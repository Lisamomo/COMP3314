{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Assignment 3 - basic classifiers\n",
    "\n",
    "Math practice and coding application for main classifiers introduced in Chapter 3 of the Python machine learning book. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Weighting\n",
    "\n",
    "Note that this assignment is more difficult than the previous ones, and thus has a higher weighting 3 and longer duration (3 weeks). Each one of the previous two assignments has a weighting 1.\n",
    "\n",
    "Specifically, the first 3 assignments contribute to your continuous assessment as follows:\n",
    "\n",
    "Assignment weights: $w_1 = 1, w_2 = 1, w_3 = 3$\n",
    "\n",
    "Assignment grades: $g_1, g_2, g_3$\n",
    "\n",
    "Weighted average: $\\frac{1}{\\sum_i w_i} \\times \\sum_i \\left(w_i \\times g_i \\right)$\n",
    "\n",
    "Future assignments will be added analogously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# RBF kernel (20 points)\n",
    "\n",
    "Show that a Gaussian RBF kernel can be expressed as a dot product:\n",
    "$$\n",
    "K(\\mathbf{x}, \\mathbf{y}) \n",
    "= e^\\frac{-|\\mathbf{x} - \\mathbf{y}|^2}{2} \n",
    "= \\phi(\\mathbf{x})^T \\phi(\\mathbf{y})\n",
    "$$\n",
    "by spelling out the mapping function $\\phi$.\n",
    "\n",
    "For simplicity\n",
    "* you can assume both $\\mathbf{x}$ and $\\mathbf{y}$ are 2D vectors\n",
    "$\n",
    "x =\n",
    "\\begin{pmatrix}\n",
    "x_1 \\\\\n",
    "x_2\n",
    "\\end{pmatrix}\n",
    ", \\;\n",
    "y =\n",
    "\\begin{pmatrix}\n",
    "y_1 \\\\\n",
    "y_2\n",
    "\\end{pmatrix}\n",
    "$\n",
    "* we use a scalar unit variance here\n",
    "\n",
    "even though the proof can be extended for vectors $\\mathbf{x}$ $\\mathbf{y}$ and general covariance matrices.\n",
    "\n",
    "Hint: use Taylor series expansion of the exponential function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$K(\\mathbf{x}, \\mathbf{y}) = e^\\frac{-|\\mathbf{x} - \\mathbf{y}|^2}{2} = e^{-\\frac{||\\mathbf{x}||^2}{2}} \\times e^{-\\frac{||\\mathbf{y}||^2}{2}} \\times e^{{x}^T {y}} = e^{-\\frac{||\\mathbf{x}||^2}{2}} \\times e^{-\\frac{||\\mathbf{y}||^2}{2}} \\times\\sum\\limits^{\\infty}_{n=0}\\frac{({x}^T {y})^n}{n!} = e^{-\\frac{||\\mathbf{x}||^2}{2}}\\times e^{-\\frac{||\\mathbf{y}||^2}{2}}\\times K_{poly}(x,y)$ \n",
    "\n",
    "$K_{poly}(x)$ corresponds to the following kernel function: \n",
    "$\\phi^*(x) = [1, x_1, x_2, \\frac{{x_1}^{2}}{\\sqrt{2}}, \\frac{{x_2}^{2}}{\\sqrt{2}},x_1x_2, ...]$\n",
    "\n",
    "$\\phi(x) = e^{-\\frac{||\\mathbf{x}||^2}{2}} \\times \\phi^*(x)= e^{-\\frac{||\\mathbf{x}||^2}{2}} \\times [1, x_1, x_2, \\frac{{x_1}^{2}}{\\sqrt{2}}, \\frac{{x_2}^{2}}{\\sqrt{2}},x_1x_2, ...]$\n",
    "\n",
    "To be specific , $\\phi^*(x) = \\left((k!) (n-k)!\\right)^{-0.5} x_1^k x_2^{n-k}$, where $ k \\in [0, n], n \\in [0, \\infty]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Kernel SVM complexity (10 points)\n",
    "\n",
    "How would the complexity (in terms of number of parameters) of a trained kernel SVM change with the amount of training data, and why?\n",
    "Note that the answer may depend on the specific kernel used as well as the amount of training data.\n",
    "Consider specifically the following types of kernels $K(\\mathbf{x}, \\mathbf{y})$.\n",
    "* linear:\n",
    "$$\n",
    "K\\left(\\mathbf{x}, \\mathbf{y}\\right) = \\mathbf{x}^T \\mathbf{y}\n",
    "$$\n",
    "* polynomial with degree $q$:\n",
    "$$\n",
    "K\\left(\\mathbf{x}, \\mathbf{y}\\right) =\n",
    "(\\mathbf{x}^T\\mathbf{y} + 1)^q\n",
    "$$\n",
    "* RBF with distance function $D$:\n",
    "$$\n",
    "K\\left(\\mathbf{x}, \\mathbf{y} \\right) = e^{-\\frac{D\\left(\\mathbf{x}, \\mathbf{y} \\right)}{2s^2}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let n be the training data and d be the dimension of x.\n",
    "\n",
    "For linear kernel, the complexity is is O(nd). The complexity will become larger linearly with the increase of amount of training data.\n",
    "\n",
    "For polynomial kernel with degree  q , the complexity is is O($nd^q$). The complexity will become larger linearly with the increase of amount of training data.\n",
    "\n",
    "For RBF with distance function  D kernel, the complexity is shown to be infinity by expressing the kernel as a dot product. The complexity will remain infinity with the change of amount of training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gaussian density Bayes (30 points)\n",
    "\n",
    "$$\n",
    "p\\left(\\Theta | \\mathbf{X}\\right)\n",
    "= \n",
    "\\frac{p\\left(\\mathbf{X} | \\Theta\\right) p\\left(\\Theta\\right)}{p\\left(\\mathbf{X}\\right)}\n",
    "$$\n",
    "\n",
    "Assume both the likelihood and prior have Gaussian distributions:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(\\mathbf{X} | \\Theta)\n",
    "&=\n",
    "\\frac{1}{(2\\pi)^{N/2}\\sigma^N} \\exp\\left(-\\frac{\\sum_{t=1}^N (\\mathbf{x}^{(t)} - \\Theta)^2}{2\\sigma^2}\\right)\n",
    "\\\\\n",
    "p(\\Theta)\n",
    "&=\n",
    "\\frac{1}{\\sqrt{2\\pi}\\sigma_0} \\exp\\left( -\\frac{(\\Theta - \\mu_0)^2}{2\\sigma_0^2} \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Derive $\\Theta$ from the dataset $\\mathbf{X}$ via the following methods:\n",
    "\n",
    "### ML (maximum likelihood) estimation \n",
    "$$\n",
    "\\Theta_{ML} = argmax_{\\Theta} p(\\mathbf{X} | \\Theta)\n",
    "$$\n",
    "\n",
    "### MAP estimation\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Theta_{MAP} \n",
    "&= \n",
    "argmax_{\\Theta} p(\\Theta | \\mathbf{X})\n",
    "\\\\\n",
    "&=\n",
    "argmax_{\\Theta} p(\\mathbf{X} | \\Theta) p(\\Theta)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### Bayes estimation\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Theta_{Bayes} \n",
    "&= \n",
    "E(\\Theta | \\mathbf{X})\n",
    "\\\\\n",
    "&= \n",
    "\\int \\Theta p(\\Theta | \\mathbf{X}) d\\Theta\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### ML (maximum likelihood) estimation\n",
    "\n",
    "\n",
    "$p(\\mathbf{X} | \\Theta )\\propto \\exp\\left(-\\frac{\\sum_{t=1}^N (\\mathbf{x}^{(t)} - \\Theta)^2}{2\\sigma^2}\\right)$\n",
    "\n",
    "\n",
    "To maximize $p(\\mathbf{X} | \\Theta ) \\Leftrightarrow$ maximize $-\\frac{\\sum_{t=1}^N (\\mathbf{x}^{(t)} - \\Theta)^2}{2\\sigma^2}\\Leftrightarrow$ minimize $\\sum_{t=1}^N (\\mathbf{x}^{(t)} - \\Theta)^2$\n",
    "\n",
    "$\\frac{\\partial}{\\partial \\Theta}\\sum_{t=1}^N (\\mathbf{x}^{(t)} - \\Theta)^2=0 \\Leftrightarrow \\Theta = \\bar{x}$\n",
    "\n",
    "### MAP estimation\n",
    "\n",
    "$p(\\mathbf{X}| \\Theta) p(\\Theta) \\propto \\exp\\left(-\\left(\\frac{\\sum_{t=1}^N (\\mathbf{x}^{(t)} - \\Theta)^2}{2\\sigma^2}+\\frac{(\\Theta - \\mu_0)^2}{2\\sigma_0^2}\\right)\\right)$\n",
    "\n",
    "\n",
    "To maximize $p(\\mathbf{X}| \\Theta) p(\\Theta) \\Leftrightarrow$ maximize $-\\left(\\frac{\\sum_{t=1}^N (\\mathbf{x}^{(t)} - \\Theta)^2}{2\\sigma^2}+\\frac{(\\Theta - \\mu_0)^2}{2\\sigma_0^2}\\right)\\Leftrightarrow$ minimize $\\frac{\\sum_{t=1}^N (\\mathbf{x}^{(t)} - \\Theta)^2}{2\\sigma^2}+\\frac{(\\Theta - \\mu_0)^2}{2\\sigma_0^2}$\n",
    "\n",
    "$\\frac{\\partial}{\\partial \\Theta}\\left(\\frac{\\sum_{t=1}^N (\\mathbf{x}^{(t)} - \\Theta)^2}{2\\sigma^2}+\\frac{(\\Theta - \\mu_0)^2}{2\\sigma_0^2}\\right)=0 \\Leftrightarrow \\Theta = \\frac{\\frac{\\mu_0}{\\sigma_0^2}+\\frac{N\\bar{x}}{\\sigma^2}}{\\frac{1}{\\sigma_0^2}+\\frac{N}{\\sigma^2}}$\n",
    "\n",
    "### Bayes estimation\n",
    "\n",
    "\n",
    "Since $\\sigma,\\sigma_0,\\mathbf{x}$ are fixed,\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(\\Theta | \\mathbf{X}) \n",
    "&\n",
    "\\propto p\\left(\\mathbf{X} | \\Theta\\right) p\\left(\\Theta\\right)\n",
    "\\\\\n",
    "&=\n",
    "\\frac{1}{(2\\pi)^{N/2}\\sigma^N} \\exp\\left(-\\frac{\\sum_{t=1}^N (\\mathbf{x}^{(t)} - \\Theta)^2}{2\\sigma^2}\\right) \\times \\frac{1}{\\sqrt{2\\pi}\\sigma_0} \\exp\\left( -\\frac{(\\Theta - \\mu_0)^2}{2\\sigma_0^2} \\right)\n",
    "\\\\\n",
    "&\\propto \n",
    "\\exp\\left(-\\frac{\\sum_{t=1}^N (\\mathbf{x}^{(t)} - \\Theta)^2}{2\\sigma^2}\\right) \\times \\exp\\left( -\\frac{(\\Theta - \\mu_0)^2}{2\\sigma_0^2} \\right)\n",
    "\\\\\n",
    "&=\n",
    "\\exp\\left(-\\left(\\frac{\\sum_{t=1}^N (\\mathbf{x}^{(t)} - \\Theta)^2}{2\\sigma^2} + \\frac{(\\Theta - \\mu_0)^2}{2\\sigma_0^2} \\right)\\right)\n",
    "\\\\\n",
    "&=\n",
    "\\exp\\left(-\\left(\\frac{\\sum_{t=1}^N (\\mathbf{x}^{(t)} - \\bar{x} + \\bar{x} - \\Theta)^2}{2\\sigma^2} + \\frac{(\\Theta - \\mu_0)^2}{2\\sigma_0^2} \\right)\\right)\n",
    "\\\\\n",
    "&=\n",
    "\\exp\\left(-\\left(\\frac{\\sum_{t=1}^N (\\mathbf{x}^{(t)} - \\bar{x})^2 + N(\\bar{x} - \\Theta)^2  + 2(\\bar{x} - \\Theta)\\sum_{t=1}^N (\\mathbf{x}^{(t)} - \\bar{x})}{2\\sigma^2}+ \\frac{(\\Theta - \\mu_0)^2}{2\\sigma_0^2} \\right)\\right)\n",
    "\\\\\n",
    "&=\n",
    "\\exp\\left(-\\left(\\frac{\\sum_{t=1}^N (\\mathbf{x}^{(t)} - \\bar{x})^2 + N(\\bar{x} - \\Theta)^2 }{2\\sigma^2}+ \\frac{(\\Theta - \\mu_0)^2}{2\\sigma_0^2} \\right)\\right)\n",
    "\\\\\n",
    "&=\n",
    "\\exp\\left(-\\frac{\\sum_{t=1}^N (\\mathbf{x}^{(t)} - \\bar{x})^2}{2\\sigma^2}\\right) \\times \\exp\\left(-\\left(\\frac{N(\\bar{x} - \\Theta)^2 }{2\\sigma^2}+ \\frac{(\\Theta - \\mu_0)^2}{2\\sigma_0^2} \\right)\\right)\n",
    "\\\\\n",
    "&\\propto\n",
    "\\exp\\left(-\\left(\\frac{N(\\bar{x} - \\Theta)^2 }{2\\sigma^2}+ \\frac{(\\Theta - \\mu_0)^2}{2\\sigma_0^2} \\right)\\right)\n",
    "\\\\\n",
    "&=\n",
    "\\exp\\left(-\\left(\\theta^2(\\frac{N}{2\\sigma^2}+\\frac{1}{2\\sigma_0^2})-\\theta(\\frac{N\\bar{x}}{\\sigma^2}+\\frac{\\mu_0}{\\sigma_0^2})+\\frac{N\\bar{x}^2}{2\\sigma^2}+\\frac{\\mu_0^2}{2\\sigma_0^2}\\right)\\right)\n",
    "\\\\\n",
    "&=\n",
    "\\exp\\left(-\\frac{1}{2}\\left(\\theta^2(\\frac{N}{\\sigma^2}+\\frac{1}{\\sigma_0^2})-2\\theta(\\frac{N\\bar{x}}{\\sigma^2}+\\frac{\\mu_0}{\\sigma_0^2})+\\frac{N\\bar{x}^2}{\\sigma^2}+\\frac{\\mu_0^2}{\\sigma_0^2}\\right)\\right)\n",
    "\\\\\n",
    "&\\propto\n",
    "\\exp\\left(-\\frac{1}{2}\\left(\\theta^2(\\frac{N}{\\sigma^2}+\\frac{1}{\\sigma_0^2})-2\\theta(\\frac{N\\bar{x}}{\\sigma^2}+\\frac{\\mu_0}{\\sigma_0^2})\\right)\\right)\n",
    "\\\\\n",
    "&\\propto\n",
    "\\exp\\left(-\\frac{(\\theta-\\mu_0^*)^2}{2{\\sigma^*}^2}\\right)\n",
    "\\\\\n",
    "&\n",
    "where \\space \\mu_0^* = \\frac{\\frac{\\mu_0}{\\sigma_0^2}+\\frac{N\\bar{x}}{\\sigma^2}}{\\frac{1}{\\sigma_0^2}+\\frac{N}{\\sigma^2}},\\space {\\sigma^*}^2=\\frac{1}{\\frac{1}{\\sigma_0^2}+\\frac{N}{\\sigma^2}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "According to the $p(\\Theta | \\mathbf{X})$, we can infer that  $\\Theta | \\mathbf{X}$ is a Normal distribution.\n",
    "Therefore, $\\Theta_{Bayes} =E(\\Theta | \\mathbf{X}) = \\mu_0^* = \\frac{\\frac{\\mu_0}{\\sigma_0^2}+\\frac{N\\bar{x}}{\\sigma^2}}{\\frac{1}{\\sigma_0^2}+\\frac{N}{\\sigma^2}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hand-written digit classification (40 points)\n",
    "\n",
    "In the textbook sample code we applied different scikit-learn classifers for the Iris data set.\n",
    "\n",
    "In this exercise, we will apply the same set of classifiers over a different data set: hand-written digits.\n",
    "Please write down the code for different classifiers, choose their hyper-parameters, and compare their performance via the accuracy score as in the Iris dataset.\n",
    "Which classifier(s) perform(s) the best and worst, and why?\n",
    "\n",
    "The classifiers include:\n",
    "* perceptron\n",
    "* logistic regression\n",
    "* SVM\n",
    "* decision tree\n",
    "* random forest\n",
    "* KNN\n",
    "* naive Bayes\n",
    "\n",
    "The dataset is available as part of scikit learn, as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n",
      "(1797,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "\n",
    "X = digits.data # training data\n",
    "y = digits.target # training label\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x206ccfc3d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVgAAAFuCAYAAADETwDwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x206ccfc3e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pylab as pl\n",
    "\n",
    "index = 13\n",
    "pl.gray()\n",
    "pl.matshow(digits.images[index])\n",
    "pl.title('digit ' + str(digits.target[index]))\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Date Preprocessing\n",
    "Hint: How you divide training and test data set? And apply other techinques we have learned if needed.\n",
    "You could take a look at the Iris data set case in the textbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#Your code comes here\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Classifier #1 Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.93\n"
     ]
    }
   ],
   "source": [
    "#Your code, including traing and testing, to observe the accuracies.\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "ppn = Perceptron(n_iter = 40, eta0 = 0.1, random_state = 0)\n",
    "_ = ppn.fit(X_train_std, y_train)\n",
    "y_pred = ppn.predict(X_test_std)\n",
    "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Classifier #2 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.94\n"
     ]
    }
   ],
   "source": [
    "#Your code, including traing and testing, to observe the accuracies.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(C = 1000, random_state = 0)\n",
    "_ = lr.fit(X_train_std, y_train)\n",
    "y_pred = lr.predict(X_test_std)\n",
    "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Classifier #3 SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "#Your code, including traing and testing, to observe the accuracies.\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(kernel = 'linear', C = 1.0, random_state = 0)\n",
    "_ = svm.fit(X_train_std, y_train)\n",
    "y_pred = svm.predict(X_test_std)\n",
    "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Classifier #4 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.82\n"
     ]
    }
   ],
   "source": [
    "#Your code, including traing and testing, to observe the accuracies.\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier(criterion = 'entropy', max_depth = 7, random_state = 0)\n",
    "_ = tree.fit(X_train_std, y_train)\n",
    "y_pred = tree.predict(X_test_std)\n",
    "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Classifer #5 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "#Your code, including traing and testing, to observe the accuracies.\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest =RandomForestClassifier(criterion='entropy', n_estimators=100, random_state=0, n_jobs=2)\n",
    "_ = forest.fit(X_train_std, y_train)\n",
    "y_pred = forest.predict(X_test_std)\n",
    "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Classifier #6 KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "#Your code, including traing and testing, to observe the accuracies.\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=10, p=2, metric='minkowski')\n",
    "knn.fit(X_train_std, y_train)\n",
    "y_pred = forest.predict(X_test_std)\n",
    "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Classifier #7 Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.77\n"
     ]
    }
   ],
   "source": [
    "#Your code, including traing and testing, to observe the accuracies.\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "_ = gnb.fit(X_train_std, y_train)\n",
    "y_pred = gnb.predict(X_test_std)\n",
    "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Among the classifiers, SVM, Random Forest, KNN perform the best and Naive Bayes perform the worst.\n",
    " \n",
    "For SVM, it maximizes margins to nearest samples and is more robust against outliers.\n",
    " \n",
    "For Random Forest, it is somehow a kind of ensemble of decision tree. It combines weak learners to build a more robust model and thus has a better generalization error and is less susceptible to overfitting.\n",
    " \n",
    "For KNN, it deterimines the class label of a new data point by a majority vote among its k nearest neighbours.\n",
    "\n",
    "However, for Naive Bayes, it assumes the features are independent for the likelihood, which might not be true in practice."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
