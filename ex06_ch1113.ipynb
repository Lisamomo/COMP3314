{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Assignment 6\n",
    "\n",
    "This assignment has weighting $3.5$.\n",
    "The first question about clustering has 35%, and the second question about tiny image classification has 65%.\n",
    "\n",
    "This is a challenging assignment, so I recommend you start early."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Clustering for handwritten digits\n",
    "\n",
    "Supervised learning requires labeled data, which can be expensive to acquire.\n",
    "For example, a dataset with $N$ samples for classification will require manual labeling $N$ times.\n",
    "\n",
    "One way to ameliorate this issue is to perform clustering of the raw data samples first, followed by manual inspection and labeling of only a few samples.\n",
    "Recall that clustering is a form of non-supervised learning, so it does not require any class labels.\n",
    "\n",
    "For example, say we are given a set of scanned hand-written digit images.\n",
    "We can cluster them into 10 groups first, manually inspect and label a few images in each cluster, and propagate the label towards the rest of all (unlabeled) samples in each cluster.\n",
    "\n",
    "The accuracy of such semi-automatic labeling depends on the accuracy of the clustering.\n",
    "If each cluster (0 to 9) corresponds exactly to hand-written digits 0-9, we are fine.\n",
    "Otherwise, we have some mis-labeled data.\n",
    "\n",
    "The goal of this question is to exercise clustering of the scikit-learn digits dataset which has labels, so that we can verify our clustering accuracy.\n",
    "The specifics are as follows.\n",
    "\n",
    "You will be judged by the test accuracy of your code, and quality of descriptions of your method.\n",
    "As a reference, a simple code I (Li-Yi) wrote can achieve about 78% accuracy. Try to beat it as much as you can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Training and test data split\n",
    "\n",
    "We will split the original dataset into training and test datasets\n",
    "* training for building our clusters\n",
    "* testing to see if the clusters can predict future data\n",
    "\n",
    "## Accuracy\n",
    "What is your clustering accuracy (comparing cluster labels with the ground truth labels), and what are the properties of mis-clustered samples?\n",
    "\n",
    "## Data preprocessing\n",
    "Would the original features (pixels) work well, or we need further processing like scaling/standardization or dimensionality-reduction, before clustering?\n",
    "\n",
    "## Models and hyper-parameters\n",
    "\n",
    "Let's focus on k-means clustering, as hierarchical and density-based clustering do not provide the predict() method under scikit-learn.\n",
    "\n",
    "What is the best test performance you can achieve with which hyper-parameters (for k-means, standard scalar, and dimensionality reduction)?\n",
    "\n",
    "### Hint\n",
    "We have learned Pipeline and GridSearchCV for cross validation and hyper-parameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last updated: 2016-12-05 \n",
      "\n",
      "CPython 3.5.2\n",
      "IPython 5.1.0\n",
      "\n",
      "numpy 1.11.2\n",
      "pandas 0.19.1\n",
      "matplotlib 1.5.3\n",
      "scipy 0.18.1\n",
      "sklearn 0.18.1\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a '' -u -d -v -p numpy,pandas,matplotlib,scipy,sklearn\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Added version check for recent scikit-learn 0.18 checks\n",
    "from distutils.version import LooseVersion as Version\n",
    "from sklearn import __version__ as sklearn_version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n",
      "(1797,)\n",
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "\n",
    "X = digits.data # data in pixels\n",
    "y = digits.target # digit labels\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "print(np.unique(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEZCAYAAADCJLEQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0G9cRSgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f37904c37b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "\n",
    "num_rows = 4\n",
    "num_cols = 5\n",
    "\n",
    "fig, ax = plt.subplots(nrows=num_rows, ncols=num_cols, sharex=True, sharey=True)\n",
    "ax = ax.flatten()\n",
    "for index in range(num_rows*num_cols):\n",
    "    img = digits.images[index]\n",
    "    label = digits.target[index]\n",
    "    ax[index].imshow(img, cmap='Greys', interpolation='nearest')\n",
    "    ax[index].set_title('digit ' + str(label))\n",
    "\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Data sets: training versus test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 1257, test: 540\n"
     ]
    }
   ],
   "source": [
    "if Version(sklearn_version) < '0.18':\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "else:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "num_training = y_train.shape[0]\n",
    "num_test = y_test.shape[0]\n",
    "print('training: ' + str(num_training) + ', test: ' + str(num_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[119 133 128 119 120 135 130 122 128 123]\n",
      "[59 49 49 64 61 47 51 57 46 57]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# check to see if the data are well distributed among digits\n",
    "for y in [y_train, y_test]:\n",
    "    print(np.bincount(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We first write a scoring function for clustering so that we can use for GridSearchCV.\n",
    "Take a look at use_scorer under scikit learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from itertools import permutations\n",
    "\n",
    "def clustering_accuracy_score(y_true, y_pred):   \n",
    "    #find how many unique labels exist in y_true and y_cm\n",
    "    y_cm = y_pred\n",
    "    index_true = np.unique(y_true)\n",
    "    index_cm = np.unique(y_cm)\n",
    "    \n",
    "    #good is used to find the perfect label for y_cm\n",
    "    good = [-1] * len(index_cm)\n",
    "  \n",
    "    #find the perfect label\n",
    "    for i in range(len(index_cm)):\n",
    "        y_true_test = y_true[y_cm == index_cm[i]]\n",
    "        good[i] = np.argmax(np.bincount(y_true_test))\n",
    "        \n",
    "    #modify the index of y_cm to reach consistency with y_true\n",
    "    good = np.array(good)\n",
    "    for i in range(len(y_cm)):\n",
    "        y_cm[i] = good[np.where(index_cm == y_cm[i])][0]\n",
    "  \n",
    "    return accuracy_score(y_true = y_true, y_pred = y_cm)    \n",
    "\n",
    "clustering_accuracy = make_scorer(clustering_accuracy_score) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 1.0 , should be 1\n",
      "accuracy 0.833333333333 , should be 0.8333333333333334\n",
      "accuracy 0.833333333333 , should be 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "# toy case demonstrating the clustering accuracy\n",
    "# this is just a reference to illustrate what this score function is trying to achieve\n",
    "# feel free to design your own as long as you can justify\n",
    "\n",
    "# ground truth class label for samples\n",
    "toy_y_true = np.array([0, 0, 0, 1, 1, 2])\n",
    "# clustering id for samples\n",
    "toy_y_pred_true = np.array([1, 1, 1, 2, 2, 0])\n",
    "toy_y_pred_bad1 = np.array([0, 0, 1, 1, 1, 2])\n",
    "toy_y_pred_bad2 = np.array([2, 2, 1, 0, 0, 0])\n",
    "\n",
    "toy_accuracy = clustering_accuracy_score(toy_y_true, toy_y_pred_true)\n",
    "print('accuracy', toy_accuracy, ', should be 1')\n",
    "\n",
    "toy_accuracy = clustering_accuracy_score(toy_y_true, toy_y_pred_bad1)\n",
    "print('accuracy', toy_accuracy, ', should be', 5.0/6.0)\n",
    "\n",
    "toy_accuracy = clustering_accuracy_score(toy_y_true, toy_y_pred_bad2)\n",
    "print('accuracy', toy_accuracy, ', should be', 4.0/6.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For toy_y_pred_bad2, the accuracy I have is different from the accuracy provided. From my point of view, \n",
    "y_cm     y_true\n",
    "0    ->    1\n",
    "1    ->    0\n",
    "2    ->    0\n",
    "Thus, y_cm turns into 0, 0, 0, 1, 1, 1. And the accuracy is 5/6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Build a pipeline with standard scaler, PCA, and clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# your code\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe_clt = Pipeline([('scl', StandardScaler()),\n",
    "                    ('pca', PCA(n_components=2)),\n",
    "                    ('clt', KMeans(n_clusters= 10, tol=1e-04))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Use GridSearchCV to tune hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clt': KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "     n_clusters=10, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "     random_state=None, tol=0.0001, verbose=0),\n",
       " 'clt__algorithm': 'auto',\n",
       " 'clt__copy_x': True,\n",
       " 'clt__init': 'k-means++',\n",
       " 'clt__max_iter': 300,\n",
       " 'clt__n_clusters': 10,\n",
       " 'clt__n_init': 10,\n",
       " 'clt__n_jobs': 1,\n",
       " 'clt__precompute_distances': 'auto',\n",
       " 'clt__random_state': None,\n",
       " 'clt__tol': 0.0001,\n",
       " 'clt__verbose': 0,\n",
       " 'pca': PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
       "   svd_solver='auto', tol=0.0, whiten=False),\n",
       " 'pca__copy': True,\n",
       " 'pca__iterated_power': 'auto',\n",
       " 'pca__n_components': 2,\n",
       " 'pca__random_state': None,\n",
       " 'pca__svd_solver': 'auto',\n",
       " 'pca__tol': 0.0,\n",
       " 'pca__whiten': False,\n",
       " 'scl': StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       " 'scl__copy': True,\n",
       " 'scl__with_mean': True,\n",
       " 'scl__with_std': True,\n",
       " 'steps': [('scl', StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "  ('pca',\n",
       "   PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
       "     svd_solver='auto', tol=0.0, whiten=False)),\n",
       "  ('clt', KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "       n_clusters=10, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "       random_state=None, tol=0.0001, verbose=0))]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to see which hyper-parameters can be tuned\n",
    "pipe_clt.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 2112 candidates, totalling 21120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 21120 out of 21120 | elapsed: 52.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 2112 candidates, totalling 21120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 21120 out of 21120 | elapsed: 49.4min finished\n"
     ]
    }
   ],
   "source": [
    "# your code\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# turn on scaling\n",
    "param_grid1 = {'scl__with_mean': [True],\n",
    "              'scl__with_std': [True],\n",
    "              'pca__n_components': np.array(range(1,X.shape[1]+1)),\n",
    "              'clt__max_iter': [100, 200, 300],\n",
    "              'clt__n_clusters': np.array(range(10,21))}          \n",
    "\n",
    "# turn off scaling\n",
    "param_grid2 = {'scl__with_mean': [False],\n",
    "              'scl__with_std': [False],\n",
    "              'pca__n_components': np.array(range(1,X.shape[1]+1)),\n",
    "              'clt__max_iter': [100, 200, 300],\n",
    "              'clt__n_clusters': np.array(range(10,21))}   \n",
    "    \n",
    "gs1 = GridSearchCV(estimator=pipe_clt,\n",
    "                  param_grid=param_grid1, \n",
    "                  scoring=clustering_accuracy, \n",
    "                  cv=10,\n",
    "                  n_jobs=1,\n",
    "                  verbose=True\n",
    "                 )\n",
    "\n",
    "gs2 = GridSearchCV(estimator=pipe_clt,\n",
    "                  param_grid=param_grid2, \n",
    "                  scoring=clustering_accuracy, \n",
    "                  cv=10,\n",
    "                  n_jobs=1,\n",
    "                  verbose=True\n",
    "                 )\n",
    "\n",
    "# previously, when I just use 1 gs to do the GridSearchCV, the result turns out that StandardScaler(copy=True, with_mean=False, with_std=True)\n",
    "# which makes no sense... \n",
    "# So I use 2 gs to test whether the data should be scaled before clustering\n",
    "gs1 = gs1.fit(X_train, y_train)\n",
    "gs2 = gs2.fit(X_train, y_train)        \n",
    "\n",
    "# below is Li-Yi's dummy code to build a random guess model\n",
    "import numpy as np\n",
    "class RandomGuesser:\n",
    "    def __init__(self, num_classes):\n",
    "        self.num_classes = num_classes\n",
    "    def predict(self, X):\n",
    "        y = np.random.randint(low = 0, high = self.num_classes, size = X.shape[0])\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scl', StandardScaler(copy=True, with_mean=False, with_std=False)), ('pca', PCA(copy=True, iterated_power='auto', n_components=55, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('clt', KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
      "    n_clusters=20, n_init=10, n_jobs=1, precompute_distances='auto',\n",
      "    random_state=None, tol=0.0001, verbose=0))])\n",
      "Test accuracy: 0.900\n"
     ]
    }
   ],
   "source": [
    "#best_model = RandomGuesser(10) # replace this with the best model you can build\n",
    "#test whether with scaling or not performs better\n",
    "if gs1.best_score_ > gs2.best_score_:\n",
    "    best_model = gs1.best_estimator_\n",
    "else:\n",
    "    best_model = gs2.best_estimator_\n",
    "\n",
    "print (best_model)\n",
    "\n",
    "best_model.fit(X_train, y_train)\n",
    "y_cm = best_model.predict(X_test)\n",
    "\n",
    "print('Test accuracy: %.3f' % clustering_accuracy_score(y_true=y_test, y_pred=y_cm))\n",
    "\n",
    "#print('Test accuracy: %.3f' % best_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Visualize mis-clustered samples, and provide your explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaYAAAMRCAYAAACnIRLnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3787750fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "import math\n",
    "\n",
    "mis_img = X_test[y_test != y_cm][:]\n",
    "correct_label = y_test[y_test != y_cm][:]\n",
    "mis_label = y_cm[y_test != y_cm][:]\n",
    "\n",
    "nrow = math.ceil(len(mis_img) / 5)\n",
    "fig, ax = plt.subplots(nrows=nrow, ncols=5, sharex=True, sharey=True,figsize = (6,nrow))\n",
    "ax = ax.flatten()\n",
    "for i in range(len(mis_img)):   \n",
    "    img = mis_img[i].reshape(8, 8)\n",
    "    ax[i].imshow(img, cmap='Greys', interpolation='nearest')\n",
    "    ax[i].set_title('%d) t: %d p: %d' % (i+1, correct_label[i], mis_label[i]))\n",
    "\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "your explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "## Accuracy\n",
    "\n",
    "My accuracy is around 90%.\n",
    "Actually, the mis-clustered samples take the form of a mixture of true value and predicted value.Sometimes, it is even difficult for human beings to distinguish and give the correct labels. For example, image36's true value should be 8. However, I cannot tell whether it is 1, 7 or 9, but never 8! Also noteworthy is that the pixels of the datasets are very low. If these samples have a higher resolution, the prediction will be improved without doubt. Furthermore, I use 20 clusters to train the data. The increasing use of the clusters can better catch characteristics of the handwriting of various people. Even for the same number, let's say 7, our writing habit are not the same.\n",
    "\n",
    "## Data preprocessing\n",
    "\n",
    "According to GridSearchCV, we don't need to go through standardization for data preprocessing. What we need to go through is dimensionality reduction According to my experiment, PCA of the best model uses 55 components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tiny image classification\n",
    "\n",
    "We will use the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html) for image object recognition.\n",
    "The dataset consists of 50000 training samples and 10000 test samples in 10 different classes (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck; see the link above for more information).\n",
    "The goal is to maximize the accuracy of your classifier on the test dataset after being optimized via the training dataset.\n",
    "\n",
    "You can use any learning models (supervised or unsupervised) or optimization methods (e.g. search methods for hyper-parameters).\n",
    "The only requirement is that your code can run inside an ipynb file, as usual.\n",
    "Please provide a description of your method, in addition to the code.\n",
    "Your answer will be evaluated not only on the test accuracy but also on the creativity of your methodology and the quality of your explanation/description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Sample code to get you started\n",
    "\n",
    "This is a difficult classification task.\n",
    "A sample code below, based on a simple fully connected neural network built via Keras, is provided below.\n",
    "The test accuracy is about 43%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last updated: 2016-12-05 \n",
      "\n",
      "CPython 3.5.2\n",
      "IPython 5.1.0\n",
      "\n",
      "numpy 1.11.2\n",
      "keras 1.1.1\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a '' -u -d -v -p numpy,keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape:  (32, 32, 3)\n",
      "50000 training samples\n",
      "10000 test samples\n",
      "10 classes\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# load data set\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "img_shape = X_train.shape[1:] # [num_rows, num_cols, num_channels]\n",
    "num_img_pixels = np.prod(img_shape)\n",
    "num_training_samples = X_train.shape[0]\n",
    "num_test_samples = X_test.shape[0]\n",
    "\n",
    "nb_classes = np.sum(np.unique(y_train).shape)\n",
    "\n",
    "print('image shape: ', img_shape)\n",
    "print(X_train.shape[0], 'training samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "print(nb_classes, 'classes')\n",
    "\n",
    "# data processing\n",
    "\n",
    "X_train = X_train.reshape(num_training_samples, num_img_pixels)\n",
    "X_test = X_test.reshape(num_test_samples, num_img_pixels)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# one hot encoding of labels\n",
    "y_train_ohe = np_utils.to_categorical(y_train)\n",
    "y_test_ohe = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# build a basic network\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(input_dim = num_img_pixels, \n",
    "                output_dim = 50, \n",
    "                init = 'uniform', \n",
    "                activation = 'tanh'))\n",
    "\n",
    "model.add(Dense(output_dim = 50, \n",
    "                init = 'uniform', \n",
    "                activation = 'tanh'))\n",
    "\n",
    "model.add(Dense(output_dim = nb_classes, \n",
    "                init = 'uniform', \n",
    "                activation = 'softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.001, decay=1e-7, momentum=.9)\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', \n",
    "              optimizer = sgd, \n",
    "              metrics = [\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# train\n",
    "\n",
    "_ = model.fit(X_train, y_train_ohe, \n",
    "              nb_epoch = 5, \n",
    "              batch_size = 10, \n",
    "              verbose = False, # turn this on to visualize progress \n",
    "              validation_split = 0.1 # 10% of training data for validation per epoch\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few predictions:  [6 1 9]\n",
      "Training accuracy: 0.43282\n",
      "Test accuracy: 0.4304\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "\n",
    "y_train_pred = model.predict_classes(X_train, verbose=False)\n",
    "print('First few predictions: ', y_train_pred[:3])\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "print('Training accuracy:', train_acc)\n",
    "\n",
    "y_test_pred = model.predict_classes(X_test, verbose=False)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "print('Test accuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape:  (32, 32, 3)\n",
      "50000 training samples\n",
      "10000 test samples\n",
      "10 classes\n"
     ]
    }
   ],
   "source": [
    "# your code and experimental results\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.optimizers import SGD\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# load data set\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "img_shape = X_train.shape[1:] # [num_rows, num_cols, num_channels]\n",
    "num_img_pixels = np.prod(img_shape)\n",
    "num_training_samples = X_train.shape[0]\n",
    "num_test_samples = X_test.shape[0]\n",
    "\n",
    "nb_classes = np.sum(np.unique(y_train).shape)\n",
    "\n",
    "print('image shape: ', img_shape)\n",
    "print(X_train.shape[0], 'training samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "print(nb_classes, 'classes')\n",
    "\n",
    "# data processing\n",
    "\n",
    "#X_train = X_train.reshape(num_training_samples, num_img_pixels)\n",
    "#X_test = X_test.reshape(num_test_samples, num_img_pixels)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# one hot encoding of labels\n",
    "y_train_ohe = np_utils.to_categorical(y_train)\n",
    "y_test_ohe = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from keras import backend as K\n",
    "# Orthonorm init code is taked from Lasagne\n",
    "# https://github.com/Lasagne/Lasagne/blob/master/lasagne/init.py\n",
    "def svd_orthonormal(shape):\n",
    "    if len(shape) < 2:\n",
    "        raise RuntimeError(\"Only shapes of length 2 or more are supported.\")\n",
    "    flat_shape = (shape[0], np.prod(shape[1:]))\n",
    "    a = np.random.standard_normal(flat_shape)\n",
    "    u, _, v = np.linalg.svd(a, full_matrices=False)\n",
    "    q = u if u.shape == flat_shape else v\n",
    "    q = q.reshape(shape)\n",
    "    return q\n",
    "\n",
    "def get_activations(model, layer, X_batch):\n",
    "    get_activations = K.function([model.layers[0].input, K.learning_phase()], [model.layers[layer].output,])\n",
    "    activations = get_activations([X_batch,0])\n",
    "    return activations\n",
    "\n",
    "def LSUVinit(model,batch):\n",
    "    margin = 0.1\n",
    "    max_iter = 10\n",
    "    i=-1\n",
    "    for layer in model.layers:\n",
    "        i+=1\n",
    "        print(layer.get_config()['name'])\n",
    "        if (('convolution' not in layer.get_config()['name']) and 'clf' not in layer.get_config()['name'] and 'dense' not in layer.get_config()['name']):\n",
    "            continue\n",
    "        w_all=layer.get_weights();\n",
    "        weights = np.array(w_all[0])\n",
    "        weights = svd_orthonormal(weights.shape)\n",
    "        biases = np.array(w_all[1])\n",
    "        w_all_new = [weights,biases]\n",
    "        layer.set_weights(w_all_new)\n",
    "        acts1=get_activations(model,i,batch)\n",
    "        var1=np.var(acts1)\n",
    "        iter1=0\n",
    "        needed_variance = 1.0\n",
    "        print(var1)\n",
    "        while (abs(needed_variance - var1) > margin):\n",
    "            w_all=layer.get_weights();\n",
    "            weights = np.array(w_all[0])\n",
    "            biases = np.array(w_all[1])\n",
    "            weights /= np.sqrt(var1)/np.sqrt(needed_variance)\n",
    "            w_all_new = [weights,biases]\n",
    "            layer.set_weights(w_all_new)\n",
    "            acts1=get_activations(model,i,batch)\n",
    "            var1=np.var(acts1)\n",
    "            iter1+=1\n",
    "            print(var1)\n",
    "            if iter1 > max_iter:\n",
    "                break\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convolution2d_1\n",
      "0.0298732\n",
      "1.0\n",
      "activation_1\n",
      "convolution2d_2\n",
      "0.0500271\n",
      "1.0\n",
      "activation_2\n",
      "convolution2d_3\n",
      "0.0386222\n",
      "1.0\n",
      "activation_3\n",
      "maxpooling2d_1\n",
      "dropout_1\n",
      "convolution2d_4\n",
      "0.0237685\n",
      "1.0\n",
      "activation_4\n",
      "convolution2d_5\n",
      "0.0196425\n",
      "1.0\n",
      "activation_5\n",
      "maxpooling2d_2\n",
      "dropout_2\n",
      "convolution2d_6\n",
      "0.0118312\n",
      "1.0\n",
      "activation_6\n",
      "convolution2d_7\n",
      "0.0103933\n",
      "1.0\n",
      "activation_7\n",
      "convolution2d_8\n",
      "0.00914927\n",
      "1.0\n",
      "activation_8\n",
      "averagepooling2d_1\n",
      "dropout_3\n",
      "flatten_1\n",
      "dense_1\n",
      "0.113806\n",
      "1.0\n",
      "activation_9\n",
      "dropout_4\n",
      "dense_2\n",
      "0.274083\n",
      "1.0\n",
      "activation_10\n"
     ]
    }
   ],
   "source": [
    "#2nd best model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(32, 3, 3, border_mode='same', input_shape=X_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(32, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(32, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Convolution2D(80, 3, 3, border_mode='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(80, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Convolution2D(128, 3, 3, border_mode='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(128, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(128, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(AveragePooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.001, decay=1e-7, momentum=.9)\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', \n",
    "              optimizer = sgd, \n",
    "              metrics = [\"accuracy\"])\n",
    "\n",
    "batch_size = 128\n",
    "model = LSUVinit(model,X_train[:batch_size,:,:,:]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/200\n",
      "45000/45000 [==============================] - 228s - loss: 2.2101 - acc: 0.1540 - val_loss: 2.1479 - val_acc: 0.1620\n",
      "Epoch 2/200\n",
      "45000/45000 [==============================] - 229s - loss: 1.9960 - acc: 0.2050 - val_loss: 1.9828 - val_acc: 0.2316\n",
      "Epoch 3/200\n",
      "45000/45000 [==============================] - 229s - loss: 1.8980 - acc: 0.2509 - val_loss: 1.8847 - val_acc: 0.2970\n",
      "Epoch 4/200\n",
      "45000/45000 [==============================] - 229s - loss: 1.8015 - acc: 0.3020 - val_loss: 1.8766 - val_acc: 0.3312\n",
      "Epoch 5/200\n",
      "45000/45000 [==============================] - 232s - loss: 1.7082 - acc: 0.3517 - val_loss: 1.6738 - val_acc: 0.3970\n",
      "Epoch 6/200\n",
      "45000/45000 [==============================] - 231s - loss: 1.6436 - acc: 0.3811 - val_loss: 1.7814 - val_acc: 0.3678\n",
      "Epoch 7/200\n",
      "45000/45000 [==============================] - 231s - loss: 1.5916 - acc: 0.4068 - val_loss: 1.5480 - val_acc: 0.4360\n",
      "Epoch 8/200\n",
      "45000/45000 [==============================] - 232s - loss: 1.5426 - acc: 0.4319 - val_loss: 1.6795 - val_acc: 0.4044\n",
      "Epoch 9/200\n",
      "45000/45000 [==============================] - 232s - loss: 1.4939 - acc: 0.4534 - val_loss: 1.4393 - val_acc: 0.4882\n",
      "Epoch 10/200\n",
      "45000/45000 [==============================] - 231s - loss: 1.4565 - acc: 0.4717 - val_loss: 1.4153 - val_acc: 0.4992\n",
      "Epoch 11/200\n",
      "45000/45000 [==============================] - 231s - loss: 1.4101 - acc: 0.4913 - val_loss: 1.2964 - val_acc: 0.5350\n",
      "Epoch 12/200\n",
      "45000/45000 [==============================] - 232s - loss: 1.3694 - acc: 0.5086 - val_loss: 1.3398 - val_acc: 0.5200\n",
      "Epoch 13/200\n",
      "45000/45000 [==============================] - 232s - loss: 1.3351 - acc: 0.5236 - val_loss: 1.3027 - val_acc: 0.5308\n",
      "Epoch 14/200\n",
      "45000/45000 [==============================] - 231s - loss: 1.2990 - acc: 0.5356 - val_loss: 1.2087 - val_acc: 0.5788\n",
      "Epoch 15/200\n",
      "45000/45000 [==============================] - 231s - loss: 1.2638 - acc: 0.5520 - val_loss: 1.0749 - val_acc: 0.6190\n",
      "Epoch 16/200\n",
      "45000/45000 [==============================] - 231s - loss: 1.2376 - acc: 0.5612 - val_loss: 1.0619 - val_acc: 0.6242\n",
      "Epoch 17/200\n",
      "45000/45000 [==============================] - 232s - loss: 1.2061 - acc: 0.5763 - val_loss: 1.1520 - val_acc: 0.5998\n",
      "Epoch 18/200\n",
      "45000/45000 [==============================] - 232s - loss: 1.1839 - acc: 0.5818 - val_loss: 1.2083 - val_acc: 0.5784\n",
      "Epoch 19/200\n",
      "45000/45000 [==============================] - 233s - loss: 1.1565 - acc: 0.5953 - val_loss: 1.0055 - val_acc: 0.6450\n",
      "Epoch 20/200\n",
      "45000/45000 [==============================] - 232s - loss: 1.1373 - acc: 0.6039 - val_loss: 1.0299 - val_acc: 0.6360\n",
      "Epoch 21/200\n",
      "45000/45000 [==============================] - 232s - loss: 1.1157 - acc: 0.6072 - val_loss: 1.0224 - val_acc: 0.6420\n",
      "Epoch 22/200\n",
      "45000/45000 [==============================] - 231s - loss: 1.0941 - acc: 0.6199 - val_loss: 0.9961 - val_acc: 0.6514\n",
      "Epoch 23/200\n",
      "45000/45000 [==============================] - 232s - loss: 1.0665 - acc: 0.6261 - val_loss: 1.0126 - val_acc: 0.6386\n",
      "Epoch 24/200\n",
      "45000/45000 [==============================] - 232s - loss: 1.0534 - acc: 0.6345 - val_loss: 0.9246 - val_acc: 0.6742\n",
      "Epoch 25/200\n",
      "45000/45000 [==============================] - 232s - loss: 1.0360 - acc: 0.6391 - val_loss: 0.8803 - val_acc: 0.6902\n",
      "Epoch 26/200\n",
      "45000/45000 [==============================] - 232s - loss: 1.0165 - acc: 0.6472 - val_loss: 0.8881 - val_acc: 0.6926\n",
      "Epoch 27/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.9970 - acc: 0.6538 - val_loss: 0.8488 - val_acc: 0.7032\n",
      "Epoch 28/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.9818 - acc: 0.6621 - val_loss: 0.8686 - val_acc: 0.6946\n",
      "Epoch 29/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.9654 - acc: 0.6643 - val_loss: 0.8616 - val_acc: 0.6938\n",
      "Epoch 30/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.9454 - acc: 0.6719 - val_loss: 0.8578 - val_acc: 0.6982\n",
      "Epoch 31/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.9339 - acc: 0.6775 - val_loss: 0.8286 - val_acc: 0.7080\n",
      "Epoch 32/200\n",
      "45000/45000 [==============================] - 234s - loss: 0.9220 - acc: 0.6830 - val_loss: 0.8307 - val_acc: 0.7124\n",
      "Epoch 33/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.9117 - acc: 0.6880 - val_loss: 0.7690 - val_acc: 0.7270\n",
      "Epoch 34/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.8951 - acc: 0.6911 - val_loss: 0.8048 - val_acc: 0.7152\n",
      "Epoch 35/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.8781 - acc: 0.6993 - val_loss: 0.8037 - val_acc: 0.7242\n",
      "Epoch 36/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.8698 - acc: 0.7010 - val_loss: 0.7296 - val_acc: 0.7462\n",
      "Epoch 37/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.8574 - acc: 0.7044 - val_loss: 0.8017 - val_acc: 0.7202\n",
      "Epoch 38/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.8451 - acc: 0.7099 - val_loss: 0.7954 - val_acc: 0.7284\n",
      "Epoch 39/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.8387 - acc: 0.7136 - val_loss: 0.9092 - val_acc: 0.6866\n",
      "Epoch 40/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.8208 - acc: 0.7186 - val_loss: 0.6972 - val_acc: 0.7566\n",
      "Epoch 41/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.8183 - acc: 0.7226 - val_loss: 0.7038 - val_acc: 0.7584\n",
      "Epoch 42/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.8110 - acc: 0.7234 - val_loss: 0.7389 - val_acc: 0.7402\n",
      "Epoch 43/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.7933 - acc: 0.7290 - val_loss: 0.7177 - val_acc: 0.7594\n",
      "Epoch 44/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.7886 - acc: 0.7312 - val_loss: 0.7224 - val_acc: 0.7488\n",
      "Epoch 45/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.7806 - acc: 0.7313 - val_loss: 0.6510 - val_acc: 0.7740\n",
      "Epoch 46/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.7748 - acc: 0.7370 - val_loss: 0.6714 - val_acc: 0.7680\n",
      "Epoch 47/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.7629 - acc: 0.7401 - val_loss: 0.7135 - val_acc: 0.7536\n",
      "Epoch 48/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.7619 - acc: 0.7380 - val_loss: 0.6549 - val_acc: 0.7748\n",
      "Epoch 49/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.7421 - acc: 0.7476 - val_loss: 0.7086 - val_acc: 0.7620\n",
      "Epoch 50/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.7321 - acc: 0.7497 - val_loss: 0.6542 - val_acc: 0.7796\n",
      "Epoch 51/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.7298 - acc: 0.7517 - val_loss: 0.6977 - val_acc: 0.7638\n",
      "Epoch 52/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.7253 - acc: 0.7538 - val_loss: 0.6787 - val_acc: 0.7702\n",
      "Epoch 53/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.7146 - acc: 0.7549 - val_loss: 0.6261 - val_acc: 0.7818\n",
      "Epoch 54/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.7087 - acc: 0.7571 - val_loss: 0.6518 - val_acc: 0.7756\n",
      "Epoch 55/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.7013 - acc: 0.7607 - val_loss: 0.6769 - val_acc: 0.7658\n",
      "Epoch 56/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.6982 - acc: 0.7620 - val_loss: 0.6416 - val_acc: 0.7790\n",
      "Epoch 57/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.6891 - acc: 0.7658 - val_loss: 0.6186 - val_acc: 0.7880\n",
      "Epoch 58/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.6843 - acc: 0.7684 - val_loss: 0.6061 - val_acc: 0.7890\n",
      "Epoch 59/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.6774 - acc: 0.7690 - val_loss: 0.6369 - val_acc: 0.7812\n",
      "Epoch 60/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.6718 - acc: 0.7704 - val_loss: 0.6147 - val_acc: 0.7902\n",
      "Epoch 61/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.6700 - acc: 0.7713 - val_loss: 0.6516 - val_acc: 0.7780\n",
      "Epoch 62/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.6646 - acc: 0.7700 - val_loss: 0.6256 - val_acc: 0.7874\n",
      "Epoch 63/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.6604 - acc: 0.7754 - val_loss: 0.5933 - val_acc: 0.7970\n",
      "Epoch 64/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.6485 - acc: 0.7794 - val_loss: 0.6373 - val_acc: 0.7808\n",
      "Epoch 65/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.6477 - acc: 0.7807 - val_loss: 0.5680 - val_acc: 0.8040\n",
      "Epoch 66/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.6380 - acc: 0.7849 - val_loss: 0.5920 - val_acc: 0.8002\n",
      "Epoch 67/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.6386 - acc: 0.7812 - val_loss: 0.6492 - val_acc: 0.7824\n",
      "Epoch 68/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.6301 - acc: 0.7855 - val_loss: 0.6078 - val_acc: 0.7932\n",
      "Epoch 69/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.6273 - acc: 0.7862 - val_loss: 0.5764 - val_acc: 0.8046\n",
      "Epoch 70/200\n",
      "45000/45000 [==============================] - 230s - loss: 0.6199 - acc: 0.7884 - val_loss: 0.6043 - val_acc: 0.7946\n",
      "Epoch 71/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.6157 - acc: 0.7916 - val_loss: 0.5929 - val_acc: 0.7970\n",
      "Epoch 72/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.6119 - acc: 0.7911 - val_loss: 0.5916 - val_acc: 0.7976\n",
      "Epoch 73/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.6060 - acc: 0.7949 - val_loss: 0.5601 - val_acc: 0.8122\n",
      "Epoch 74/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.5989 - acc: 0.7947 - val_loss: 0.5796 - val_acc: 0.8070\n",
      "Epoch 75/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.5943 - acc: 0.7984 - val_loss: 0.5631 - val_acc: 0.8066\n",
      "Epoch 76/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.5933 - acc: 0.7955 - val_loss: 0.5726 - val_acc: 0.8060\n",
      "Epoch 77/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.5903 - acc: 0.7996 - val_loss: 0.5457 - val_acc: 0.8180\n",
      "Epoch 78/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.5898 - acc: 0.7996 - val_loss: 0.5668 - val_acc: 0.8124\n",
      "Epoch 79/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.5818 - acc: 0.8024 - val_loss: 0.5807 - val_acc: 0.8058\n",
      "Epoch 80/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.5773 - acc: 0.8024 - val_loss: 0.5944 - val_acc: 0.7982\n",
      "Epoch 81/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.5744 - acc: 0.8043 - val_loss: 0.5433 - val_acc: 0.8138\n",
      "Epoch 82/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.5687 - acc: 0.8071 - val_loss: 0.5412 - val_acc: 0.8160\n",
      "Epoch 83/200\n",
      "45000/45000 [==============================] - 230s - loss: 0.5656 - acc: 0.8084 - val_loss: 0.5337 - val_acc: 0.8186\n",
      "Epoch 84/200\n",
      "45000/45000 [==============================] - 230s - loss: 0.5579 - acc: 0.8097 - val_loss: 0.5532 - val_acc: 0.8150\n",
      "Epoch 85/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.5551 - acc: 0.8122 - val_loss: 0.5247 - val_acc: 0.8212\n",
      "Epoch 86/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.5523 - acc: 0.8113 - val_loss: 0.5476 - val_acc: 0.8130\n",
      "Epoch 87/200\n",
      "45000/45000 [==============================] - 230s - loss: 0.5422 - acc: 0.8145 - val_loss: 0.5285 - val_acc: 0.8188\n",
      "Epoch 88/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.5416 - acc: 0.8139 - val_loss: 0.5201 - val_acc: 0.8258\n",
      "Epoch 89/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.5425 - acc: 0.8155 - val_loss: 0.5401 - val_acc: 0.8160\n",
      "Epoch 90/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.5424 - acc: 0.8136 - val_loss: 0.5556 - val_acc: 0.8082\n",
      "Epoch 91/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.5342 - acc: 0.8158 - val_loss: 0.5238 - val_acc: 0.8198\n",
      "Epoch 92/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.5318 - acc: 0.8190 - val_loss: 0.5823 - val_acc: 0.8092\n",
      "Epoch 93/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.5309 - acc: 0.8192 - val_loss: 0.5519 - val_acc: 0.8130\n",
      "Epoch 94/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.5237 - acc: 0.8211 - val_loss: 0.5319 - val_acc: 0.8194\n",
      "Epoch 95/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.5175 - acc: 0.8227 - val_loss: 0.5194 - val_acc: 0.8212\n",
      "Epoch 96/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.5186 - acc: 0.8222 - val_loss: 0.5293 - val_acc: 0.8236\n",
      "Epoch 97/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.5159 - acc: 0.8244 - val_loss: 0.5305 - val_acc: 0.8252\n",
      "Epoch 98/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.5105 - acc: 0.8253 - val_loss: 0.5092 - val_acc: 0.8260\n",
      "Epoch 99/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.5063 - acc: 0.8270 - val_loss: 0.5587 - val_acc: 0.8132\n",
      "Epoch 100/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.5109 - acc: 0.8245 - val_loss: 0.5471 - val_acc: 0.8154\n",
      "Epoch 101/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.5020 - acc: 0.8259 - val_loss: 0.5032 - val_acc: 0.8236\n",
      "Epoch 102/200\n",
      "45000/45000 [==============================] - 230s - loss: 0.4952 - acc: 0.8302 - val_loss: 0.5207 - val_acc: 0.8224\n",
      "Epoch 103/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.4993 - acc: 0.8294 - val_loss: 0.5165 - val_acc: 0.8234\n",
      "Epoch 104/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.4926 - acc: 0.8314 - val_loss: 0.5067 - val_acc: 0.8276\n",
      "Epoch 105/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.4942 - acc: 0.8299 - val_loss: 0.5019 - val_acc: 0.8284\n",
      "Epoch 106/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.4911 - acc: 0.8326 - val_loss: 0.5114 - val_acc: 0.8198\n",
      "Epoch 107/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.4829 - acc: 0.8357 - val_loss: 0.5009 - val_acc: 0.8262\n",
      "Epoch 108/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.4821 - acc: 0.8349 - val_loss: 0.4842 - val_acc: 0.8318\n",
      "Epoch 109/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.4823 - acc: 0.8353 - val_loss: 0.5592 - val_acc: 0.8158\n",
      "Epoch 110/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.4778 - acc: 0.8349 - val_loss: 0.4926 - val_acc: 0.8296\n",
      "Epoch 111/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.4733 - acc: 0.8376 - val_loss: 0.5073 - val_acc: 0.8300\n",
      "Epoch 112/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.4721 - acc: 0.8379 - val_loss: 0.4829 - val_acc: 0.8290\n",
      "Epoch 113/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.4649 - acc: 0.8416 - val_loss: 0.5388 - val_acc: 0.8214\n",
      "Epoch 114/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.4671 - acc: 0.8406 - val_loss: 0.4771 - val_acc: 0.8334\n",
      "Epoch 115/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.4627 - acc: 0.8414 - val_loss: 0.5023 - val_acc: 0.8294\n",
      "Epoch 116/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.4676 - acc: 0.8407 - val_loss: 0.5223 - val_acc: 0.8236\n",
      "Epoch 117/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.4547 - acc: 0.8449 - val_loss: 0.4777 - val_acc: 0.8400\n",
      "Epoch 118/200\n",
      "45000/45000 [==============================] - 230s - loss: 0.4520 - acc: 0.8466 - val_loss: 0.5101 - val_acc: 0.8338\n",
      "Epoch 119/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.4519 - acc: 0.8478 - val_loss: 0.5013 - val_acc: 0.8310\n",
      "Epoch 120/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.4520 - acc: 0.8451 - val_loss: 0.5053 - val_acc: 0.8296\n",
      "Epoch 121/200\n",
      "45000/45000 [==============================] - 230s - loss: 0.4432 - acc: 0.8489 - val_loss: 0.5078 - val_acc: 0.8280\n",
      "Epoch 122/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.4416 - acc: 0.8491 - val_loss: 0.4890 - val_acc: 0.8304\n",
      "Epoch 123/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.4456 - acc: 0.8470 - val_loss: 0.5045 - val_acc: 0.8256\n",
      "Epoch 124/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.4430 - acc: 0.8497 - val_loss: 0.5068 - val_acc: 0.8306\n",
      "Epoch 125/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.4337 - acc: 0.8515 - val_loss: 0.4921 - val_acc: 0.8324\n",
      "Epoch 126/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.4337 - acc: 0.8523 - val_loss: 0.4957 - val_acc: 0.8296\n",
      "Epoch 127/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.4302 - acc: 0.8502 - val_loss: 0.4889 - val_acc: 0.8322\n",
      "Epoch 128/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.4315 - acc: 0.8521 - val_loss: 0.4705 - val_acc: 0.8420\n",
      "Epoch 129/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.4277 - acc: 0.8529 - val_loss: 0.4867 - val_acc: 0.8390\n",
      "Epoch 130/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.4260 - acc: 0.8554 - val_loss: 0.4752 - val_acc: 0.8368\n",
      "Epoch 131/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.4276 - acc: 0.8552 - val_loss: 0.5043 - val_acc: 0.8308\n",
      "Epoch 132/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.4152 - acc: 0.8561 - val_loss: 0.4827 - val_acc: 0.8360\n",
      "Epoch 133/200\n",
      "45000/45000 [==============================] - 230s - loss: 0.4195 - acc: 0.8553 - val_loss: 0.4944 - val_acc: 0.8344\n",
      "Epoch 134/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.4164 - acc: 0.8560 - val_loss: 0.4897 - val_acc: 0.8342\n",
      "Epoch 135/200\n",
      "45000/45000 [==============================] - 230s - loss: 0.4186 - acc: 0.8562 - val_loss: 0.4764 - val_acc: 0.8404\n",
      "Epoch 136/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.4161 - acc: 0.8586 - val_loss: 0.4921 - val_acc: 0.8386\n",
      "Epoch 137/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.4102 - acc: 0.8591 - val_loss: 0.4757 - val_acc: 0.8394\n",
      "Epoch 138/200\n",
      "45000/45000 [==============================] - 233s - loss: 0.4052 - acc: 0.8596 - val_loss: 0.4755 - val_acc: 0.8374\n",
      "Epoch 139/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.4036 - acc: 0.8649 - val_loss: 0.4585 - val_acc: 0.8462\n",
      "Epoch 140/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.4018 - acc: 0.8619 - val_loss: 0.4846 - val_acc: 0.8336\n",
      "Epoch 141/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.4022 - acc: 0.8636 - val_loss: 0.4911 - val_acc: 0.8330\n",
      "Epoch 142/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.4052 - acc: 0.8619 - val_loss: 0.4730 - val_acc: 0.8368\n",
      "Epoch 143/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.4026 - acc: 0.8625 - val_loss: 0.4983 - val_acc: 0.8300\n",
      "Epoch 144/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.4050 - acc: 0.8616 - val_loss: 0.4805 - val_acc: 0.8404\n",
      "Epoch 145/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.3951 - acc: 0.8644 - val_loss: 0.4747 - val_acc: 0.8416\n",
      "Epoch 146/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.3904 - acc: 0.8656 - val_loss: 0.4697 - val_acc: 0.8434\n",
      "Epoch 147/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.3893 - acc: 0.8655 - val_loss: 0.4807 - val_acc: 0.8412\n",
      "Epoch 148/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.3941 - acc: 0.8646 - val_loss: 0.4823 - val_acc: 0.8366\n",
      "Epoch 149/200\n",
      "45000/45000 [==============================] - 230s - loss: 0.3820 - acc: 0.8699 - val_loss: 0.5055 - val_acc: 0.8320\n",
      "Epoch 150/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.3827 - acc: 0.8696 - val_loss: 0.4960 - val_acc: 0.8342\n",
      "Epoch 151/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.3860 - acc: 0.8685 - val_loss: 0.4704 - val_acc: 0.8446\n",
      "Epoch 152/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.3799 - acc: 0.8691 - val_loss: 0.4680 - val_acc: 0.8444\n",
      "Epoch 153/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.3805 - acc: 0.8684 - val_loss: 0.4899 - val_acc: 0.8360\n",
      "Epoch 154/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.3775 - acc: 0.8698 - val_loss: 0.4577 - val_acc: 0.8428\n",
      "Epoch 155/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.3799 - acc: 0.8692 - val_loss: 0.4563 - val_acc: 0.8438\n",
      "Epoch 156/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.3678 - acc: 0.8735 - val_loss: 0.4709 - val_acc: 0.8436\n",
      "Epoch 157/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.3722 - acc: 0.8725 - val_loss: 0.4747 - val_acc: 0.8432\n",
      "Epoch 158/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.3643 - acc: 0.8755 - val_loss: 0.4810 - val_acc: 0.8402\n",
      "Epoch 159/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.3729 - acc: 0.8716 - val_loss: 0.4716 - val_acc: 0.8428\n",
      "Epoch 160/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.3635 - acc: 0.8753 - val_loss: 0.5023 - val_acc: 0.8356\n",
      "Epoch 161/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.3743 - acc: 0.8700 - val_loss: 0.4775 - val_acc: 0.8356\n",
      "Epoch 162/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.3621 - acc: 0.8760 - val_loss: 0.4750 - val_acc: 0.8384\n",
      "Epoch 163/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.3692 - acc: 0.8739 - val_loss: 0.4523 - val_acc: 0.8470\n",
      "Epoch 164/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.3602 - acc: 0.8751 - val_loss: 0.4675 - val_acc: 0.8430\n",
      "Epoch 165/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.3566 - acc: 0.8775 - val_loss: 0.4878 - val_acc: 0.8410\n",
      "Epoch 166/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.3619 - acc: 0.8767 - val_loss: 0.4698 - val_acc: 0.8432\n",
      "Epoch 167/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.3541 - acc: 0.8760 - val_loss: 0.4544 - val_acc: 0.8454\n",
      "Epoch 168/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.3599 - acc: 0.8769 - val_loss: 0.4777 - val_acc: 0.8352\n",
      "Epoch 169/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.3598 - acc: 0.8769 - val_loss: 0.4816 - val_acc: 0.8424\n",
      "Epoch 170/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.3506 - acc: 0.8786 - val_loss: 0.4812 - val_acc: 0.8378\n",
      "Epoch 171/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.3435 - acc: 0.8803 - val_loss: 0.4841 - val_acc: 0.8364\n",
      "Epoch 172/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.3506 - acc: 0.8784 - val_loss: 0.4710 - val_acc: 0.8406\n",
      "Epoch 173/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.3462 - acc: 0.8793 - val_loss: 0.4805 - val_acc: 0.8418\n",
      "Epoch 174/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.3450 - acc: 0.8807 - val_loss: 0.4716 - val_acc: 0.8414\n",
      "Epoch 175/200\n",
      "45000/45000 [==============================] - 233s - loss: 0.3448 - acc: 0.8811 - val_loss: 0.4728 - val_acc: 0.8476\n",
      "Epoch 176/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.3387 - acc: 0.8828 - val_loss: 0.4800 - val_acc: 0.8412\n",
      "Epoch 177/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.3434 - acc: 0.8819 - val_loss: 0.4631 - val_acc: 0.8442\n",
      "Epoch 178/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.3415 - acc: 0.8826 - val_loss: 0.4544 - val_acc: 0.8454\n",
      "Epoch 179/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.3434 - acc: 0.8823 - val_loss: 0.4521 - val_acc: 0.8490\n",
      "Epoch 180/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.3274 - acc: 0.8884 - val_loss: 0.4725 - val_acc: 0.8436\n",
      "Epoch 181/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.3337 - acc: 0.8842 - val_loss: 0.4849 - val_acc: 0.8384\n",
      "Epoch 182/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.3313 - acc: 0.8840 - val_loss: 0.4993 - val_acc: 0.8370\n",
      "Epoch 183/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.3364 - acc: 0.8832 - val_loss: 0.4912 - val_acc: 0.8366\n",
      "Epoch 184/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.3300 - acc: 0.8876 - val_loss: 0.4939 - val_acc: 0.8344\n",
      "Epoch 185/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.3251 - acc: 0.8879 - val_loss: 0.4692 - val_acc: 0.8412\n",
      "Epoch 186/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.3324 - acc: 0.8846 - val_loss: 0.4562 - val_acc: 0.8448\n",
      "Epoch 187/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.3286 - acc: 0.8884 - val_loss: 0.4623 - val_acc: 0.8480\n",
      "Epoch 188/200\n",
      "45000/45000 [==============================] - 233s - loss: 0.3192 - acc: 0.8900 - val_loss: 0.4650 - val_acc: 0.8432\n",
      "Epoch 189/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.3228 - acc: 0.8885 - val_loss: 0.4963 - val_acc: 0.8348\n",
      "Epoch 190/200\n",
      "45000/45000 [==============================] - 230s - loss: 0.3236 - acc: 0.8891 - val_loss: 0.4700 - val_acc: 0.8416\n",
      "Epoch 191/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.3197 - acc: 0.8906 - val_loss: 0.4608 - val_acc: 0.8494\n",
      "Epoch 192/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.3231 - acc: 0.8877 - val_loss: 0.4879 - val_acc: 0.8384\n",
      "Epoch 193/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.3217 - acc: 0.8886 - val_loss: 0.4509 - val_acc: 0.8524\n",
      "Epoch 194/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.3191 - acc: 0.8905 - val_loss: 0.4697 - val_acc: 0.8482\n",
      "Epoch 195/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.3149 - acc: 0.8910 - val_loss: 0.4710 - val_acc: 0.8432\n",
      "Epoch 196/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.3142 - acc: 0.8904 - val_loss: 0.4764 - val_acc: 0.8422\n",
      "Epoch 197/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.3102 - acc: 0.8919 - val_loss: 0.4626 - val_acc: 0.8476\n",
      "Epoch 198/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.3054 - acc: 0.8949 - val_loss: 0.4644 - val_acc: 0.8460\n",
      "Epoch 199/200\n",
      "45000/45000 [==============================] - 232s - loss: 0.3149 - acc: 0.8926 - val_loss: 0.4845 - val_acc: 0.8410\n",
      "Epoch 200/200\n",
      "45000/45000 [==============================] - 231s - loss: 0.3077 - acc: 0.8924 - val_loss: 0.4857 - val_acc: 0.8378\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "\n",
    "_ = model.fit(X_train, y_train_ohe, \n",
    "              nb_epoch = 200, \n",
    "              verbose = True, # turn this on to visualize progress \n",
    "              validation_split = 0.1 # 10% of training data for validation per epoch\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 86s    \n",
      "First few predictions:  [6 9 9]\n",
      "Training accuracy: 0.94348\n",
      "10000/10000 [==============================] - 17s    \n",
      "Test accuracy: 0.8339\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "\n",
    "y_train_pred = model.predict_classes(X_train, verbose=True)\n",
    "print('First few predictions: ', y_train_pred[:3])\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "print('Training accuracy:', train_acc)\n",
    "\n",
    "y_test_pred = model.predict_classes(X_test, verbose=True)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convolution2d_9\n",
      "0.0257019\n",
      "1.0\n",
      "activation_11\n",
      "convolution2d_10\n",
      "0.0280275\n",
      "1.0\n",
      "activation_12\n",
      "maxpooling2d_3\n",
      "dropout_5\n",
      "convolution2d_11\n",
      "0.019394\n",
      "1.0\n",
      "activation_13\n",
      "convolution2d_12\n",
      "0.02157\n",
      "1.0\n",
      "activation_14\n",
      "maxpooling2d_4\n",
      "dropout_6\n",
      "convolution2d_13\n",
      "0.0152608\n",
      "1.0\n",
      "activation_15\n",
      "convolution2d_14\n",
      "0.0114568\n",
      "1.0\n",
      "activation_16\n",
      "averagepooling2d_2\n",
      "dropout_7\n",
      "flatten_2\n",
      "dense_3\n",
      "0.123782\n",
      "1.0\n",
      "activation_17\n",
      "dropout_8\n",
      "dense_4\n",
      "0.416896\n",
      "1.0\n",
      "activation_18\n"
     ]
    }
   ],
   "source": [
    "#best model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(32, 3, 3, border_mode='same', input_shape=X_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(32, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Convolution2D(80, 3, 3, border_mode='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(80, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Convolution2D(128, 3, 3, border_mode='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(128, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(AveragePooling2D(pool_size=(4, 4)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "sgd = SGD(lr=0.001, decay=1e-7, momentum=.9)\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', \n",
    "              optimizer = sgd, \n",
    "              metrics = [\"accuracy\"])\n",
    "\n",
    "batch_size = 128\n",
    "model = LSUVinit(model,X_train[:batch_size,:,:,:]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/200\n",
      "45000/45000 [==============================] - 188s - loss: 2.2762 - acc: 0.1377 - val_loss: 2.1663 - val_acc: 0.1968\n",
      "Epoch 2/200\n",
      "45000/45000 [==============================] - 189s - loss: 2.0679 - acc: 0.1958 - val_loss: 1.9274 - val_acc: 0.2284\n",
      "Epoch 3/200\n",
      "45000/45000 [==============================] - 189s - loss: 1.9046 - acc: 0.2484 - val_loss: 1.9463 - val_acc: 0.2518\n",
      "Epoch 4/200\n",
      "45000/45000 [==============================] - 188s - loss: 1.8130 - acc: 0.2861 - val_loss: 1.7168 - val_acc: 0.3300\n",
      "Epoch 5/200\n",
      "45000/45000 [==============================] - 188s - loss: 1.7293 - acc: 0.3224 - val_loss: 1.6462 - val_acc: 0.3714\n",
      "Epoch 6/200\n",
      "45000/45000 [==============================] - 189s - loss: 1.6520 - acc: 0.3606 - val_loss: 1.5845 - val_acc: 0.3988\n",
      "Epoch 7/200\n",
      "45000/45000 [==============================] - 188s - loss: 1.5881 - acc: 0.3894 - val_loss: 1.6027 - val_acc: 0.4062\n",
      "Epoch 8/200\n",
      "45000/45000 [==============================] - 189s - loss: 1.5379 - acc: 0.4165 - val_loss: 1.4502 - val_acc: 0.4586\n",
      "Epoch 9/200\n",
      "45000/45000 [==============================] - 189s - loss: 1.4943 - acc: 0.4390 - val_loss: 1.5229 - val_acc: 0.4466\n",
      "Epoch 10/200\n",
      "45000/45000 [==============================] - 189s - loss: 1.4514 - acc: 0.4615 - val_loss: 1.3506 - val_acc: 0.5034\n",
      "Epoch 11/200\n",
      "45000/45000 [==============================] - 190s - loss: 1.4193 - acc: 0.4789 - val_loss: 1.2934 - val_acc: 0.5320\n",
      "Epoch 12/200\n",
      "45000/45000 [==============================] - 190s - loss: 1.3766 - acc: 0.4945 - val_loss: 1.2380 - val_acc: 0.5354\n",
      "Epoch 13/200\n",
      "45000/45000 [==============================] - 191s - loss: 1.3433 - acc: 0.5098 - val_loss: 1.2363 - val_acc: 0.5450\n",
      "Epoch 14/200\n",
      "45000/45000 [==============================] - 191s - loss: 1.3122 - acc: 0.5240 - val_loss: 1.1285 - val_acc: 0.5892\n",
      "Epoch 15/200\n",
      "45000/45000 [==============================] - 190s - loss: 1.2774 - acc: 0.5373 - val_loss: 1.1526 - val_acc: 0.5808\n",
      "Epoch 16/200\n",
      "45000/45000 [==============================] - 191s - loss: 1.2577 - acc: 0.5457 - val_loss: 1.1009 - val_acc: 0.6048\n",
      "Epoch 17/200\n",
      "45000/45000 [==============================] - 190s - loss: 1.2389 - acc: 0.5528 - val_loss: 1.1493 - val_acc: 0.5866\n",
      "Epoch 18/200\n",
      "45000/45000 [==============================] - 190s - loss: 1.2080 - acc: 0.5658 - val_loss: 1.0679 - val_acc: 0.6176\n",
      "Epoch 19/200\n",
      "45000/45000 [==============================] - 191s - loss: 1.1945 - acc: 0.5701 - val_loss: 1.0300 - val_acc: 0.6354\n",
      "Epoch 20/200\n",
      "45000/45000 [==============================] - 191s - loss: 1.1664 - acc: 0.5852 - val_loss: 1.0771 - val_acc: 0.6048\n",
      "Epoch 21/200\n",
      "45000/45000 [==============================] - 191s - loss: 1.1537 - acc: 0.5877 - val_loss: 1.0464 - val_acc: 0.6280\n",
      "Epoch 22/200\n",
      "45000/45000 [==============================] - 190s - loss: 1.1357 - acc: 0.5941 - val_loss: 0.9835 - val_acc: 0.6502\n",
      "Epoch 23/200\n",
      "45000/45000 [==============================] - 190s - loss: 1.1207 - acc: 0.6010 - val_loss: 0.9341 - val_acc: 0.6656\n",
      "Epoch 24/200\n",
      "45000/45000 [==============================] - 190s - loss: 1.1064 - acc: 0.6061 - val_loss: 0.9184 - val_acc: 0.6736\n",
      "Epoch 25/200\n",
      "45000/45000 [==============================] - 190s - loss: 1.0876 - acc: 0.6128 - val_loss: 0.8862 - val_acc: 0.6786\n",
      "Epoch 26/200\n",
      "45000/45000 [==============================] - 190s - loss: 1.0776 - acc: 0.6157 - val_loss: 0.8927 - val_acc: 0.6780\n",
      "Epoch 27/200\n",
      "45000/45000 [==============================] - 190s - loss: 1.0592 - acc: 0.6230 - val_loss: 0.9365 - val_acc: 0.6658\n",
      "Epoch 28/200\n",
      "45000/45000 [==============================] - 190s - loss: 1.0481 - acc: 0.6278 - val_loss: 0.9325 - val_acc: 0.6620\n",
      "Epoch 29/200\n",
      "45000/45000 [==============================] - 190s - loss: 1.0353 - acc: 0.6353 - val_loss: 0.9038 - val_acc: 0.6724\n",
      "Epoch 30/200\n",
      "45000/45000 [==============================] - 190s - loss: 1.0258 - acc: 0.6357 - val_loss: 0.8298 - val_acc: 0.7014\n",
      "Epoch 31/200\n",
      "45000/45000 [==============================] - 190s - loss: 1.0098 - acc: 0.6437 - val_loss: 0.8622 - val_acc: 0.6940\n",
      "Epoch 32/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.9978 - acc: 0.6466 - val_loss: 0.8151 - val_acc: 0.7128\n",
      "Epoch 33/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.9888 - acc: 0.6495 - val_loss: 0.8791 - val_acc: 0.6956\n",
      "Epoch 34/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.9731 - acc: 0.6568 - val_loss: 0.8046 - val_acc: 0.7150\n",
      "Epoch 35/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.9670 - acc: 0.6574 - val_loss: 0.7537 - val_acc: 0.7312\n",
      "Epoch 36/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.9567 - acc: 0.6616 - val_loss: 0.8403 - val_acc: 0.7046\n",
      "Epoch 37/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.9492 - acc: 0.6645 - val_loss: 0.7872 - val_acc: 0.7210\n",
      "Epoch 38/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.9326 - acc: 0.6720 - val_loss: 0.7595 - val_acc: 0.7316\n",
      "Epoch 39/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.9279 - acc: 0.6730 - val_loss: 0.7702 - val_acc: 0.7286\n",
      "Epoch 40/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.9200 - acc: 0.6789 - val_loss: 0.8193 - val_acc: 0.7160\n",
      "Epoch 41/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.9095 - acc: 0.6793 - val_loss: 0.7744 - val_acc: 0.7300\n",
      "Epoch 42/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.9022 - acc: 0.6838 - val_loss: 0.7512 - val_acc: 0.7324\n",
      "Epoch 43/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.8919 - acc: 0.6878 - val_loss: 0.6806 - val_acc: 0.7552\n",
      "Epoch 44/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.8879 - acc: 0.6910 - val_loss: 0.7080 - val_acc: 0.7554\n",
      "Epoch 45/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.8776 - acc: 0.6924 - val_loss: 0.7043 - val_acc: 0.7562\n",
      "Epoch 46/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.8699 - acc: 0.6962 - val_loss: 0.7017 - val_acc: 0.7522\n",
      "Epoch 47/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.8629 - acc: 0.6997 - val_loss: 0.7086 - val_acc: 0.7506\n",
      "Epoch 48/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.8542 - acc: 0.7019 - val_loss: 0.6823 - val_acc: 0.7636\n",
      "Epoch 49/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.8494 - acc: 0.7027 - val_loss: 0.7141 - val_acc: 0.7544\n",
      "Epoch 50/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.8454 - acc: 0.7074 - val_loss: 0.6604 - val_acc: 0.7678\n",
      "Epoch 51/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.8363 - acc: 0.7097 - val_loss: 0.7323 - val_acc: 0.7466\n",
      "Epoch 52/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.8337 - acc: 0.7089 - val_loss: 0.6725 - val_acc: 0.7686\n",
      "Epoch 53/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.8250 - acc: 0.7116 - val_loss: 0.6422 - val_acc: 0.7760\n",
      "Epoch 54/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.8209 - acc: 0.7149 - val_loss: 0.6456 - val_acc: 0.7790\n",
      "Epoch 55/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.8139 - acc: 0.7179 - val_loss: 0.6495 - val_acc: 0.7738\n",
      "Epoch 56/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.8114 - acc: 0.7205 - val_loss: 0.6254 - val_acc: 0.7800\n",
      "Epoch 57/200\n",
      "45000/45000 [==============================] - 189s - loss: 0.8098 - acc: 0.7181 - val_loss: 0.6974 - val_acc: 0.7630\n",
      "Epoch 58/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.7969 - acc: 0.7244 - val_loss: 0.6641 - val_acc: 0.7724\n",
      "Epoch 59/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.7874 - acc: 0.7273 - val_loss: 0.6526 - val_acc: 0.7720\n",
      "Epoch 60/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.7853 - acc: 0.7268 - val_loss: 0.6468 - val_acc: 0.7756\n",
      "Epoch 61/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.7755 - acc: 0.7312 - val_loss: 0.6111 - val_acc: 0.7878\n",
      "Epoch 62/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.7817 - acc: 0.7300 - val_loss: 0.6713 - val_acc: 0.7680\n",
      "Epoch 63/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.7725 - acc: 0.7330 - val_loss: 0.6460 - val_acc: 0.7826\n",
      "Epoch 64/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.7705 - acc: 0.7336 - val_loss: 0.6447 - val_acc: 0.7802\n",
      "Epoch 65/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.7621 - acc: 0.7341 - val_loss: 0.6030 - val_acc: 0.7886\n",
      "Epoch 66/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.7546 - acc: 0.7402 - val_loss: 0.6075 - val_acc: 0.7900\n",
      "Epoch 67/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.7522 - acc: 0.7399 - val_loss: 0.5883 - val_acc: 0.7904\n",
      "Epoch 68/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.7463 - acc: 0.7431 - val_loss: 0.6325 - val_acc: 0.7836\n",
      "Epoch 69/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.7416 - acc: 0.7440 - val_loss: 0.5991 - val_acc: 0.7932\n",
      "Epoch 70/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.7395 - acc: 0.7458 - val_loss: 0.6068 - val_acc: 0.7906\n",
      "Epoch 71/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.7388 - acc: 0.7436 - val_loss: 0.6563 - val_acc: 0.7790\n",
      "Epoch 72/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.7337 - acc: 0.7481 - val_loss: 0.6012 - val_acc: 0.7944\n",
      "Epoch 73/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.7246 - acc: 0.7495 - val_loss: 0.6217 - val_acc: 0.7896\n",
      "Epoch 74/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.7254 - acc: 0.7488 - val_loss: 0.6168 - val_acc: 0.7908\n",
      "Epoch 75/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.7161 - acc: 0.7530 - val_loss: 0.5880 - val_acc: 0.7976\n",
      "Epoch 76/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.7123 - acc: 0.7535 - val_loss: 0.6220 - val_acc: 0.7844\n",
      "Epoch 77/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.7164 - acc: 0.7541 - val_loss: 0.5782 - val_acc: 0.8000\n",
      "Epoch 78/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.7155 - acc: 0.7537 - val_loss: 0.5922 - val_acc: 0.7980\n",
      "Epoch 79/200\n",
      "45000/45000 [==============================] - 189s - loss: 0.7044 - acc: 0.7579 - val_loss: 0.5973 - val_acc: 0.7984\n",
      "Epoch 80/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.7025 - acc: 0.7596 - val_loss: 0.5992 - val_acc: 0.7958\n",
      "Epoch 81/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.7003 - acc: 0.7589 - val_loss: 0.6160 - val_acc: 0.7954\n",
      "Epoch 82/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.6927 - acc: 0.7604 - val_loss: 0.5940 - val_acc: 0.7942\n",
      "Epoch 83/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.6946 - acc: 0.7580 - val_loss: 0.5517 - val_acc: 0.8092\n",
      "Epoch 84/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.6885 - acc: 0.7644 - val_loss: 0.5575 - val_acc: 0.8098\n",
      "Epoch 85/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.6822 - acc: 0.7666 - val_loss: 0.5576 - val_acc: 0.8078\n",
      "Epoch 86/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.6795 - acc: 0.7654 - val_loss: 0.5431 - val_acc: 0.8118\n",
      "Epoch 87/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.6745 - acc: 0.7689 - val_loss: 0.5774 - val_acc: 0.8052\n",
      "Epoch 88/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.6691 - acc: 0.7715 - val_loss: 0.5672 - val_acc: 0.8098\n",
      "Epoch 89/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.6749 - acc: 0.7662 - val_loss: 0.5623 - val_acc: 0.8044\n",
      "Epoch 90/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.6658 - acc: 0.7722 - val_loss: 0.5695 - val_acc: 0.8070\n",
      "Epoch 91/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.6632 - acc: 0.7704 - val_loss: 0.5513 - val_acc: 0.8104\n",
      "Epoch 92/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.6592 - acc: 0.7744 - val_loss: 0.5856 - val_acc: 0.8012\n",
      "Epoch 93/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.6571 - acc: 0.7743 - val_loss: 0.5437 - val_acc: 0.8196\n",
      "Epoch 94/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.6601 - acc: 0.7734 - val_loss: 0.5570 - val_acc: 0.8140\n",
      "Epoch 95/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.6537 - acc: 0.7746 - val_loss: 0.5734 - val_acc: 0.8098\n",
      "Epoch 96/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.6468 - acc: 0.7772 - val_loss: 0.5542 - val_acc: 0.8104\n",
      "Epoch 97/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.6381 - acc: 0.7796 - val_loss: 0.5748 - val_acc: 0.8082\n",
      "Epoch 98/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.6395 - acc: 0.7785 - val_loss: 0.5931 - val_acc: 0.8032\n",
      "Epoch 99/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.6403 - acc: 0.7792 - val_loss: 0.5732 - val_acc: 0.8064\n",
      "Epoch 100/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.6382 - acc: 0.7797 - val_loss: 0.5541 - val_acc: 0.8106\n",
      "Epoch 101/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.6353 - acc: 0.7817 - val_loss: 0.5367 - val_acc: 0.8144\n",
      "Epoch 102/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.6337 - acc: 0.7835 - val_loss: 0.5674 - val_acc: 0.8096\n",
      "Epoch 103/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.6272 - acc: 0.7852 - val_loss: 0.5485 - val_acc: 0.8138\n",
      "Epoch 104/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.6284 - acc: 0.7831 - val_loss: 0.5751 - val_acc: 0.8060\n",
      "Epoch 105/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.6288 - acc: 0.7835 - val_loss: 0.5184 - val_acc: 0.8272\n",
      "Epoch 106/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.6208 - acc: 0.7861 - val_loss: 0.5922 - val_acc: 0.8018\n",
      "Epoch 107/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.6170 - acc: 0.7876 - val_loss: 0.5261 - val_acc: 0.8220\n",
      "Epoch 108/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.6143 - acc: 0.7898 - val_loss: 0.5355 - val_acc: 0.8200\n",
      "Epoch 109/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.6173 - acc: 0.7894 - val_loss: 0.5397 - val_acc: 0.8194\n",
      "Epoch 110/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.6104 - acc: 0.7914 - val_loss: 0.5077 - val_acc: 0.8326\n",
      "Epoch 111/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.6088 - acc: 0.7903 - val_loss: 0.5259 - val_acc: 0.8232\n",
      "Epoch 112/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.6109 - acc: 0.7899 - val_loss: 0.5324 - val_acc: 0.8210\n",
      "Epoch 113/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.6063 - acc: 0.7902 - val_loss: 0.5232 - val_acc: 0.8226\n",
      "Epoch 114/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.5956 - acc: 0.7925 - val_loss: 0.5581 - val_acc: 0.8168\n",
      "Epoch 115/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.6005 - acc: 0.7942 - val_loss: 0.5377 - val_acc: 0.8232\n",
      "Epoch 116/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.6001 - acc: 0.7920 - val_loss: 0.5233 - val_acc: 0.8240\n",
      "Epoch 117/200\n",
      "45000/45000 [==============================] - 189s - loss: 0.5999 - acc: 0.7949 - val_loss: 0.5031 - val_acc: 0.8278\n",
      "Epoch 118/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.5931 - acc: 0.7970 - val_loss: 0.5275 - val_acc: 0.8268\n",
      "Epoch 119/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.5895 - acc: 0.7954 - val_loss: 0.5097 - val_acc: 0.8288\n",
      "Epoch 120/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.5898 - acc: 0.7971 - val_loss: 0.5552 - val_acc: 0.8166\n",
      "Epoch 121/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.5861 - acc: 0.7998 - val_loss: 0.5242 - val_acc: 0.8248\n",
      "Epoch 122/200\n",
      "45000/45000 [==============================] - 189s - loss: 0.5849 - acc: 0.7971 - val_loss: 0.5022 - val_acc: 0.8316\n",
      "Epoch 123/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.5793 - acc: 0.8005 - val_loss: 0.4960 - val_acc: 0.8370\n",
      "Epoch 124/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.5861 - acc: 0.7982 - val_loss: 0.5180 - val_acc: 0.8280\n",
      "Epoch 125/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.5768 - acc: 0.8023 - val_loss: 0.5182 - val_acc: 0.8286\n",
      "Epoch 126/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.5750 - acc: 0.8012 - val_loss: 0.5138 - val_acc: 0.8264\n",
      "Epoch 127/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.5720 - acc: 0.8040 - val_loss: 0.4892 - val_acc: 0.8362\n",
      "Epoch 128/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.5721 - acc: 0.8044 - val_loss: 0.5009 - val_acc: 0.8318\n",
      "Epoch 129/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.5729 - acc: 0.8032 - val_loss: 0.5021 - val_acc: 0.8286\n",
      "Epoch 130/200\n",
      "45000/45000 [==============================] - 192s - loss: 0.5729 - acc: 0.8015 - val_loss: 0.4854 - val_acc: 0.8338\n",
      "Epoch 131/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.5658 - acc: 0.8044 - val_loss: 0.5019 - val_acc: 0.8342\n",
      "Epoch 132/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.5629 - acc: 0.8054 - val_loss: 0.5444 - val_acc: 0.8206\n",
      "Epoch 133/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.5613 - acc: 0.8090 - val_loss: 0.5292 - val_acc: 0.8268\n",
      "Epoch 134/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.5654 - acc: 0.8053 - val_loss: 0.4956 - val_acc: 0.8380\n",
      "Epoch 135/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.5586 - acc: 0.8078 - val_loss: 0.4714 - val_acc: 0.8418\n",
      "Epoch 136/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.5587 - acc: 0.8077 - val_loss: 0.4879 - val_acc: 0.8354\n",
      "Epoch 137/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.5542 - acc: 0.8094 - val_loss: 0.5250 - val_acc: 0.8296\n",
      "Epoch 138/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.5577 - acc: 0.8078 - val_loss: 0.4852 - val_acc: 0.8428\n",
      "Epoch 139/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.5529 - acc: 0.8111 - val_loss: 0.4730 - val_acc: 0.8426\n",
      "Epoch 140/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.5485 - acc: 0.8121 - val_loss: 0.4781 - val_acc: 0.8398\n",
      "Epoch 141/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.5460 - acc: 0.8127 - val_loss: 0.4919 - val_acc: 0.8398\n",
      "Epoch 142/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.5441 - acc: 0.8131 - val_loss: 0.4822 - val_acc: 0.8404\n",
      "Epoch 143/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.5402 - acc: 0.8137 - val_loss: 0.5220 - val_acc: 0.8230\n",
      "Epoch 144/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.5424 - acc: 0.8148 - val_loss: 0.4861 - val_acc: 0.8390\n",
      "Epoch 145/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.5418 - acc: 0.8155 - val_loss: 0.4813 - val_acc: 0.8396\n",
      "Epoch 146/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.5428 - acc: 0.8146 - val_loss: 0.4684 - val_acc: 0.8424\n",
      "Epoch 147/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.5362 - acc: 0.8141 - val_loss: 0.5086 - val_acc: 0.8356\n",
      "Epoch 148/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.5354 - acc: 0.8154 - val_loss: 0.4804 - val_acc: 0.8408\n",
      "Epoch 149/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.5324 - acc: 0.8170 - val_loss: 0.4837 - val_acc: 0.8392\n",
      "Epoch 150/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.5347 - acc: 0.8139 - val_loss: 0.4877 - val_acc: 0.8382\n",
      "Epoch 151/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.5325 - acc: 0.8153 - val_loss: 0.4875 - val_acc: 0.8380\n",
      "Epoch 152/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.5291 - acc: 0.8181 - val_loss: 0.5111 - val_acc: 0.8322\n",
      "Epoch 153/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.5299 - acc: 0.8176 - val_loss: 0.4936 - val_acc: 0.8350\n",
      "Epoch 154/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.5220 - acc: 0.8188 - val_loss: 0.4801 - val_acc: 0.8408\n",
      "Epoch 155/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.5313 - acc: 0.8178 - val_loss: 0.4852 - val_acc: 0.8366\n",
      "Epoch 156/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.5218 - acc: 0.8196 - val_loss: 0.4746 - val_acc: 0.8418\n",
      "Epoch 157/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.5236 - acc: 0.8201 - val_loss: 0.4912 - val_acc: 0.8414\n",
      "Epoch 158/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.5209 - acc: 0.8208 - val_loss: 0.4710 - val_acc: 0.8434\n",
      "Epoch 159/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.5254 - acc: 0.8188 - val_loss: 0.4657 - val_acc: 0.8450\n",
      "Epoch 160/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.5174 - acc: 0.8217 - val_loss: 0.4629 - val_acc: 0.8468\n",
      "Epoch 161/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.5138 - acc: 0.8228 - val_loss: 0.4530 - val_acc: 0.8458\n",
      "Epoch 162/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.5169 - acc: 0.8225 - val_loss: 0.4646 - val_acc: 0.8450\n",
      "Epoch 163/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.5135 - acc: 0.8218 - val_loss: 0.4727 - val_acc: 0.8470\n",
      "Epoch 164/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.5143 - acc: 0.8234 - val_loss: 0.4815 - val_acc: 0.8438\n",
      "Epoch 165/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.5146 - acc: 0.8226 - val_loss: 0.4720 - val_acc: 0.8498\n",
      "Epoch 166/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.5138 - acc: 0.8240 - val_loss: 0.4683 - val_acc: 0.8472\n",
      "Epoch 167/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.5066 - acc: 0.8250 - val_loss: 0.4739 - val_acc: 0.8460\n",
      "Epoch 168/200\n",
      "45000/45000 [==============================] - 192s - loss: 0.5052 - acc: 0.8247 - val_loss: 0.4841 - val_acc: 0.8380\n",
      "Epoch 169/200\n",
      "45000/45000 [==============================] - 192s - loss: 0.5078 - acc: 0.8256 - val_loss: 0.4731 - val_acc: 0.8446\n",
      "Epoch 170/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.5013 - acc: 0.8278 - val_loss: 0.4735 - val_acc: 0.8410\n",
      "Epoch 171/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.5026 - acc: 0.8270 - val_loss: 0.4624 - val_acc: 0.8468\n",
      "Epoch 172/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.5035 - acc: 0.8255 - val_loss: 0.4616 - val_acc: 0.8452\n",
      "Epoch 173/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.4986 - acc: 0.8274 - val_loss: 0.4699 - val_acc: 0.8440\n",
      "Epoch 174/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.5007 - acc: 0.8262 - val_loss: 0.4926 - val_acc: 0.8402\n",
      "Epoch 175/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.4977 - acc: 0.8298 - val_loss: 0.4877 - val_acc: 0.8348\n",
      "Epoch 176/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.4951 - acc: 0.8293 - val_loss: 0.4634 - val_acc: 0.8464\n",
      "Epoch 177/200\n",
      "45000/45000 [==============================] - 189s - loss: 0.4958 - acc: 0.8284 - val_loss: 0.4727 - val_acc: 0.8462\n",
      "Epoch 178/200\n",
      "45000/45000 [==============================] - 189s - loss: 0.4972 - acc: 0.8288 - val_loss: 0.4690 - val_acc: 0.8434\n",
      "Epoch 179/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.4958 - acc: 0.8288 - val_loss: 0.4612 - val_acc: 0.8508\n",
      "Epoch 180/200\n",
      "45000/45000 [==============================] - 189s - loss: 0.4852 - acc: 0.8321 - val_loss: 0.4724 - val_acc: 0.8450\n",
      "Epoch 181/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.4875 - acc: 0.8332 - val_loss: 0.4664 - val_acc: 0.8432\n",
      "Epoch 182/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.4923 - acc: 0.8287 - val_loss: 0.4663 - val_acc: 0.8470\n",
      "Epoch 183/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.4840 - acc: 0.8341 - val_loss: 0.4441 - val_acc: 0.8528\n",
      "Epoch 184/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.4877 - acc: 0.8328 - val_loss: 0.4674 - val_acc: 0.8434\n",
      "Epoch 185/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.4885 - acc: 0.8329 - val_loss: 0.4761 - val_acc: 0.8430\n",
      "Epoch 186/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.4844 - acc: 0.8327 - val_loss: 0.4584 - val_acc: 0.8472\n",
      "Epoch 187/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.4830 - acc: 0.8338 - val_loss: 0.4553 - val_acc: 0.8468\n",
      "Epoch 188/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.4747 - acc: 0.8370 - val_loss: 0.4585 - val_acc: 0.8452\n",
      "Epoch 189/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.4771 - acc: 0.8377 - val_loss: 0.4474 - val_acc: 0.8498\n",
      "Epoch 190/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.4802 - acc: 0.8350 - val_loss: 0.4606 - val_acc: 0.8486\n",
      "Epoch 191/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.4759 - acc: 0.8358 - val_loss: 0.4461 - val_acc: 0.8510\n",
      "Epoch 192/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.4807 - acc: 0.8334 - val_loss: 0.4504 - val_acc: 0.8476\n",
      "Epoch 193/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.4750 - acc: 0.8381 - val_loss: 0.4461 - val_acc: 0.8484\n",
      "Epoch 194/200\n",
      "45000/45000 [==============================] - 190s - loss: 0.4765 - acc: 0.8339 - val_loss: 0.4530 - val_acc: 0.8492\n",
      "Epoch 195/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.4695 - acc: 0.8368 - val_loss: 0.4714 - val_acc: 0.8442\n",
      "Epoch 196/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.4725 - acc: 0.8360 - val_loss: 0.4488 - val_acc: 0.8554\n",
      "Epoch 197/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.4700 - acc: 0.8375 - val_loss: 0.4469 - val_acc: 0.8498\n",
      "Epoch 198/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.4753 - acc: 0.8347 - val_loss: 0.4554 - val_acc: 0.8496\n",
      "Epoch 199/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.4706 - acc: 0.8366 - val_loss: 0.4858 - val_acc: 0.8420\n",
      "Epoch 200/200\n",
      "45000/45000 [==============================] - 191s - loss: 0.4661 - acc: 0.8399 - val_loss: 0.4549 - val_acc: 0.8528\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "\n",
    "_ = model.fit(X_train, y_train_ohe, \n",
    "              nb_epoch = 200, \n",
    "              verbose = True, # turn this on to visualize progress \n",
    "              validation_split = 0.1 # 10% of training data for validation per epoch\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 73s    \n",
      "First few predictions:  [6 9 9]\n",
      "Training accuracy: 0.91238\n",
      "10000/10000 [==============================] - 14s    \n",
      "Test accuracy: 0.8398\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "\n",
    "y_train_pred = model.predict_classes(X_train, verbose=True)\n",
    "print('First few predictions: ', y_train_pred[:3])\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "print('Training accuracy:', train_acc)\n",
    "\n",
    "y_test_pred = model.predict_classes(X_test, verbose=True)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Your description\n",
    "with the following main parts:\n",
    "\n",
    "<dl>\n",
    "\n",
    "<dt>Introduction</dt>\n",
    "<dd>\n",
    "At a high-level, what are the approaches you considered and why?\n",
    "</dd>\n",
    "\n",
    "<dt>Method</dt>\n",
    "<dd>\n",
    "Describe your entire pipeline, such as data-preprocessing, model selection, and hyper-parameter optpimization.\n",
    "</dd>\n",
    "\n",
    "<dt>Results</dt>\n",
    "<dd>\n",
    "Describe the experimental process you took to arrive at the solution.\n",
    "For example:\n",
    "(1) compare your approach against other approach(es) you have tried, as\n",
    "well as the MLP baseline classifer.\n",
    "(2) compare against different settings of model parameters, e.g. regularization type/strength, number of hidden units or structure of a neural network, types of kernel in a SVM, etc. \n",
    "\n",
    "<dt>Conclusion</dt>\n",
    "<dd>\n",
    "Summarize what you have learned from the experiments and discuss the limitations and potential future improvements of your current method.\n",
    "</dd>\n",
    "\n",
    "<dt>References</dt>\n",
    "<dd>\n",
    "Cite any publically available code, blog posts, research papers, etc. you used or got ideas from.\n",
    "</dl>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "I consider using CNN to train my data. Since CNN is sparser and implicitly regularized than MLP. With the aid of CNN, I can go deeper into the analysis.\n",
    "\n",
    "## Method\n",
    "\n",
    "I use LAYER-SEQUENTIAL UNIT-VARIANCE INITIALIZATION (LSUV) method to process(initialize) my data. Basically speaking, the method is made up of two steps. First, pre-initialize weights of each convolution or inner-product layer with orthonormal matrices and Gaussian noise with unit variance. Decompose the weights to orthonormal basis with QR or SVDdecomposition and replace them.  Second, normalize the variance of the output of each layer to be equal to one. This method can be considered as an orthonormal initialization combined with batch normalization performed only on the first mini-batch. It is said the initial orthonormalization of weights efficiently de-correlates layer activations and relevant experiments also show that such operation is of high efficiency compared with full batch normalization.\n",
    "\n",
    "For the layer, I referred to \"FitNet-4\" structure. Since it is said LSUV works quite well under this architecture according to the paper. However, I simplified the structure quite a lot to gain a rough idea and most importantly, to save time. \n",
    "\n",
    "Stage 1, 32*3*3 Convolution twice, a 2*2 MaxPooling and a Dropout(0.5)\n",
    "\n",
    "Stage 2, 80*3*3 Convolution twice, a 2*2 MaxPooling and a Dropout(0.5)\n",
    "\n",
    "Stage 3, 128*3*3 Convolution twice, a 4*4 AveragePooling and a Dropout(0.5)\n",
    "\n",
    "Stage 4, flatten, dense, a Dropout(0.5) and softmax\n",
    "\n",
    "## Results\n",
    "\n",
    "I made quite a lot of experiments regarding this question. \n",
    "#### 1. Randomly use a model through internet to gain some insights. The dropout is 0.25.  \n",
    "    Model can be referred at https://github.com/fchollet/keras/blob/master/examples/cifar10_cnn.py  \n",
    "    Unfortunately, I lost the training accuracy and test accuracy...  \n",
    "    But mark down that the training accuracy was quite high while the test accuracy was quite low.  \n",
    "    Seems like an issues of overfitting.  \n",
    "    \n",
    "#### 2. Randomly use a model through internet to gain some insights. The dropout is 0.25.  \n",
    "    Model can be referred at https://github.com/fchollet/keras/blob/master/examples/cifar10_cnn.py  \n",
    "    This time, I added the LSUV to process the data.  \n",
    "    Training accuracy of the first few predictions is 0.90696, while the test accuracy is 0.8033.  \n",
    "    Both accuracies were higher than first trial. Seems like LSUV works.  \n",
    "    Still an issues of overfitting.\n",
    "    \n",
    "#### 3.  Use a simplified FitNet-4. Use LSUV. The dropout is 0.5.\n",
    "\n",
    "    Stage 1, 32*3*3 Convolution twice, a 2*2 MaxPooling and a Dropout(0.5)\n",
    "    \n",
    "    Stage 2, 80*3*3 Convolution twice, a 2*2 MaxPooling and a Dropout(0.5)\n",
    "    \n",
    "    Stage 3, 128*3*3 Convolution twice, a 4*4 AveragePooling and a Dropout(0.5)\n",
    "    \n",
    "    Stage 4, flatten, dense, a Dropout(0.5) and softmax\n",
    "    \n",
    "    Model can be referred at https://arxiv.org/pdf/1511.06422v7.pdf  \n",
    "    Training accuracy of the first few predictions is 0.91238, while the test accuracy is 0.8398.  \n",
    "    Test accuracy was higher than previous trial.Seems like FitNet-4 works.  \n",
    "    Issue of overfitting gets improvement.\n",
    "    \n",
    "#### 4. Use a simplified FitNet-4. Use LSUV. The dropout is 0.5.\n",
    "\n",
    "    Stage 1, 32*3*3 Convolution twice, 48*3*3 Convolution once, a 2*2 MaxPooling and a Dropout(0.5)\n",
    "    \n",
    "    Stage 2, 80*3*3 Convolution twice, a 2*2 MaxPooling and a Dropout(0.5)\n",
    "    \n",
    "    Stage 3, 128*3*3 Convolution twice, a GlobalMaxPooling and a Dropout(0.5)\n",
    "    \n",
    "    Stage 4, dense, a Dropout(0.5) and softmax\n",
    "    \n",
    "    Model can be referred at https://arxiv.org/pdf/1511.06422v7.pdf  \n",
    "    Didn't finish the training. According to the observation for the first 20 epochs, the training efficiency was quite low.  \n",
    "    For the third trial, val_acc: 0.6588  \n",
    "    For this trial, val_acc: 0.1964  \n",
    "    I suppose the reason behind that is the use of GlobalMaxPooling, which is for spatial data. According to FitNet-4, global pooling should be applied at the last step. However, since my simplified model structure did not consist so many layers, thus the data after processed  was not that sparse. Thus, I decide that GlobalMaxPooling should not be adopted in my model.\n",
    "\n",
    "#### 5. Use a simplified FitNet-4. But add several more layers compared to the one of the third trial. Use LSUV. The dropout is 0.5.\n",
    "\n",
    "    Stage 1, 32*3*3 Convolution trice, a 2*2 MaxPooling and a Dropout(0.5)\n",
    "    \n",
    "    Stage 2, 80*3*3 Convolution twice, a 2*2 MaxPooling and a Dropout(0.5)\n",
    "    \n",
    "    Stage 3, 128*3*3 Convolution trice, a 2*2 AveragePooling and a Dropout(0.5)\n",
    "    \n",
    "    Stage 4, flatten, dense, a Dropout(0.5) and softmax\n",
    "    \n",
    "    Model can be referred at https://arxiv.org/pdf/1511.06422v7.pdf  \n",
    "    Training accuracy of the first few predictions is 0.94348, while the test accuracy is 0.8339.  \n",
    "    Test accuracy is almost the same compared to the third trial. However, the training accuracy is too high.  \n",
    "    Issue of overfitting.\n",
    "   \n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The third and the fifth trial perform the best among what I have tried. Compared to the third model, the fifth one has more layers and is more complicated. The time to train the data using this model is also higher as expected. However, the result of test accuracy is not as high as the third one. Therefore, I consider the third model to be the best, due to less training time and higher test accuracy. I learn from the experiment that test accuracy won't definitely increase along with the number of layers used in the model. And the issue of overfitting should be fixed so as to improve the test accuracy. I also acquire some information from other papers that Bayesian Optimization can be used to process the data and improve the training result (https://arxiv.org/pdf/1502.05700v2.pdf). Maybe I can combine the Bayesian Optimization into my model next time. \n",
    "\n",
    "\n",
    "## References\n",
    "1. <fchollet/keras> https://github.com/fchollet/keras/blob/master/examples/cifar10_cnn.py\n",
    "2. <ALL YOU NEED IS A GOOD INIT> https://arxiv.org/pdf/1511.06422v7.pdf\n",
    "3. <Scalable Bayesian Optimization Using Deep Neural Networks> https://arxiv.org/pdf/1502.05700v2.pdf\n",
    "4. <ducha-aiki/LSUV-keras> https://github.com/ducha-aiki/LSUV-keras  \n",
    "    @ARTICLE{LSUVInit2015,\n",
    "    author = {{Mishkin}, D. and {Matas}, J.},\n",
    "    title = \"{All you need is a good init}\",\n",
    "    journal = {arXiv preprint arXiv:1511.06422},\n",
    "    year = 2015,\n",
    "    month = nov\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "name": "_merged"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
